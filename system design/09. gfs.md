<!-- TOC -->
* [Design Google File System (GFS)](#design-google-file-system-gfs)
  * [Resources](#resources)
  * [The Problem & The Architecture](#the-problem--the-architecture)
    * [1. The Design Goals (Why GFS exists)](#1-the-design-goals-why-gfs-exists)
    * [2. The Architecture Components](#2-the-architecture-components)
      * [A. The Chunk (The Unit of Storage)](#a-the-chunk-the-unit-of-storage)
      * [B. The Three Actors](#b-the-three-actors)
    * [3. The "Single Master" Controversy](#3-the-single-master-controversy)
  * [Metadata Management & Persistence](#metadata-management--persistence)
    * [1. What Metadata does the Master store?](#1-what-metadata-does-the-master-store)
    * [2. The "In Memory" Strategy](#2-the-in-memory-strategy)
    * [3. The "Inversion of Authority" (Why Chunk Locations aren't saved)](#3-the-inversion-of-authority-why-chunk-locations-arent-saved)
    * [4. Persistence & Recovery (How to survive a crash)](#4-persistence--recovery-how-to-survive-a-crash)
      * [A. The Operation Log ( The Journal)](#a-the-operation-log--the-journal)
      * [B. Checkpointing (The Snapshot)](#b-checkpointing-the-snapshot)
      * [C. Recovery Process](#c-recovery-process)
  * [Master Operations](#master-operations)
    * [1. Namespace Management & Locking](#1-namespace-management--locking)
      * [1. The Data Structure: Hash Map vs. Inodes](#1-the-data-structure-hash-map-vs-inodes)
      * [2. The Locking Strategy (Reader-Writer Locks)](#2-the-locking-strategy-reader-writer-locks)
      * [3. The Locking Rule (Path Traversal)](#3-the-locking-rule-path-traversal)
      * [4. Detailed Examples](#4-detailed-examples)
        * [Example A: High Concurrency File Creation](#example-a-high-concurrency-file-creation)
        * [Example B: Snapshotting (The Conflict)](#example-b-snapshotting-the-conflict)
      * [5. Deadlock Prevention](#5-deadlock-prevention)
    * [2 Replica Management](#2-replica-management)
      * [1. Replica Placement (The "Rack-Aware" Strategy)](#1-replica-placement-the-rack-aware-strategy)
      * [2. Replica Creation (Choosing the "Best" Server)](#2-replica-creation-choosing-the-best-server)
      * [3. Re-Replication (The "Healing" Priority Queue)](#3-re-replication-the-healing-priority-queue)
      * [4. Replica Rebalancing (The "Optimization" Sweep)](#4-replica-rebalancing-the-optimization-sweep)
    * [3. Stale Replica Detection](#3-stale-replica-detection)
      * [1. The Core Concept: Version Numbers](#1-the-core-concept-version-numbers)
      * [2. The Trigger: Granting a Lease](#2-the-trigger-granting-a-lease)
      * [3. Detailed Example Scenario (Step-by-Step)](#3-detailed-example-scenario-step-by-step)
      * [4. The Edge Case: Client Caching](#4-the-edge-case-client-caching)
  * [Anatomy of a Read Operation](#anatomy-of-a-read-operation)
    * [Step 1: Local Translation (Client Side Math)](#step-1-local-translation-client-side-math)
    * [Step 2: Metadata Request (The Control Flow)](#step-2-metadata-request-the-control-flow)
    * [Step 3: Master Reply with "Lookahead"](#step-3-master-reply-with-lookahead)
    * [Step 4: Caching (The Memory)](#step-4-caching-the-memory)
    * [Step 5: The "Closest" Replica (The Data Flow)](#step-5-the-closest-replica-the-data-flow)
    * [Step 6: Data Transfer](#step-6-data-transfer)
    * [Summary of the "Separation of Flows"](#summary-of-the-separation-of-flows)
  * [Section 4: Storage Efficiency & Optimizations](#section-4-storage-efficiency--optimizations)
    * [1. Lazy Space Allocation (The "Expandable Folder")](#1-lazy-space-allocation-the-expandable-folder)
    * [2. The "Small File" Problem (Hotspots)](#2-the-small-file-problem-hotspots)
  * [Section 5: Anatomy of Operations (Deep Dive)](#section-5-anatomy-of-operations-deep-dive)
    * [1. Anatomy of a Read Operation](#1-anatomy-of-a-read-operation)
    * [2. Anatomy of a Write Operation](#2-anatomy-of-a-write-operation)
    * [3. Anatomy of a Record Append (The "Atomic" Special)](#3-anatomy-of-a-record-append-the-atomic-special)
    * [4. GFS Consistency Model](#4-gfs-consistency-model)
  * [Section 6: Fault Tolerance & High Availability](#section-6-fault-tolerance--high-availability)
    * [1. High Availability (Keeping the lights on)](#1-high-availability-keeping-the-lights-on)
      * [A. Chunk Replication](#a-chunk-replication)
      * [B. Fast Recovery](#b-fast-recovery)
    * [2. Master Failure (The "Brain" Death)](#2-master-failure-the-brain-death)
      * [A. The Operation Log (The Backup Brain)](#a-the-operation-log-the-backup-brain)
      * [B. Shadow Masters (Read-Only Mode)](#b-shadow-masters-read-only-mode)
    * [3. 3. Replica Failure & Stale Data Detection](#3-3-replica-failure--stale-data-detection)
    * [4. Data Integrity (Checksums)](#4-data-integrity-checksums)
  * [Section 7: Garbage Collection & Maintenance.](#section-7-garbage-collection--maintenance)
    * [1. Lazy Deletion (The "Trash Bin" Strategy)](#1-lazy-deletion-the-trash-bin-strategy)
      * [Why do we need this?](#why-do-we-need-this)
      * [The Workflow (Step-by-Step):](#the-workflow-step-by-step)
    * [2. Orphaned Chunk Collection (Physical Deletion)](#2-orphaned-chunk-collection-physical-deletion)
      * [The Workflow (The Heartbeat Handshake):](#the-workflow-the-heartbeat-handshake)
<!-- TOC -->

# Design Google File System (GFS)

## Resources

* [Google File System - Paper that inspired Hadoop](https://www.youtube.com/watch?v=eRgFNW4QFDc)

---

## The Problem & The Architecture

### 1. The Design Goals (Why GFS exists)

GFS was not built to be a "better" general-purpose file system. It was built to solve a specific
Google problem in the early 2000s.

* **Problem A: Hardware Fails Constantly.**
    * Google used thousands of cheap, "comThe text notes that system capacity is limited by the Master's RAMmodity" servers.
    * *Assumption:* At any moment, some hard drive, power supply, or network switch is broken. 
     The software **must** handle this automatically.


* **Problem B: Files are Huge.**
    * Web crawls are multi-gigabyte or terabyte files.
    * Standard file systems (4KB blocks) crumble under this weight (too much metadata).


* **Problem C: Write Once, Read Many.**
    * Data is rarely overwritten. It is usually appended to (logs) or read sequentially (scanning the web).
    * *Decision:* Optimize for **Append** and **Sequential Read**. Random write is supported but not optimized.


---

### 2. The Architecture Components

GFS uses a **Master-Slave** architecture.

#### A. The Chunk (The Unit of Storage)

Instead of small blocks (4KB), GFS uses massive blocks.

* **Size:** **64 MB**.
* **Identification:** Each chunk has a unique 64-bit **Chunk Handle**.
* **Storage:** Chunks are stored as regular Linux files on the servers.
* **Why 64 MB?** (Key Interview Question)
  1. **Reduces Metadata:** Fewer chunks mean the Master can keep *all* metadata in RAM.
  2. **Reduces Network Overhead:** Clients can keep a persistent TCP connection to a server for longer.
  3. **Reduces Chatter:** Clients ask the Master for location once, then read 64MB of data without 
    bothering the Master again.



#### B. The Three Actors

| Component           | Role                                                                                                           | State Storage                                                       |
|---------------------|----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|
| **1. The Master**   | **The Coordinator.** Stores metadata (file names, mapping). Does NOT store data.                               | **RAM** (Fast access). Persisted to disk via **Operation Log**.     |
| **2. Chunkservers** | **The Workers.** Store the actual 64MB chunks. Read/Write bits.                                                | **Linux Disk.**                                                     |
| **3. The Client**   | **The Library.** Linked into apps. Talks to Master for metadata, then talks directly to Chunkservers for data. | **RAM Cache.** Caches metadata (where chunks are) for a short time. |

---

### 3. The "Single Master" Controversy

**Question:** *Isn't a single master a bottleneck?*

**Answer:** It *would* be, if the data flowed through it.

* **The Fix:** GFS separates **Control Flow** from **Data Flow**.
  * The Master only sends tiny metadata packets (e.g., "Chunk 1 is on Server A").
  * The heavy data (Gigabytes) flows directly from Chunkserver to  Client.
  * This allows one Master to coordinate thousands of servers without getting overwhelmed.


---

## Metadata Management & Persistence

### 1. What Metadata does the Master store?

The Master tracks three specific types of data. It is crucial to know which ones are saved to disk and which are not.

| Metadata Type                | Description                                                           | Stored on Disk?      |
|------------------------------|-----------------------------------------------------------------------|----------------------|
| **1. Namespace**             | The directory tree (e.g., `/home/user/foo.txt`).                      | **YES** (Persistent) |
| **2. File-to-Chunk Mapping** | Which chunks belong to which file (e.g., `foo.txt` = Chunk #55, #56). | **YES** (Persistent) |
| **3. Chunk Locations**       | Which servers hold Chunk #55? (e.g., `Server A, Server B`).           | **NO** (Volatile)    |

### 2. The "In Memory" Strategy

The Master keeps **all** metadata in main memory (RAM).

* **Why?** It makes operations like namespace lookup and "Garbage Collection" scans instantaneous.
* **The Limit:** System capacity is theoretically limited by the Master's RAM. However, since it only takes ~64 bytes 
  of metadata to track a massive 64MB chunk, this is rarely a bottleneck in practice.

---

### 3. The "Inversion of Authority" (Why Chunk Locations aren't saved)

This is a specific design choice highlighted in the text: **"The master does not keep a persistent record
of which ChunkServers have a replica."**

* **The Logic:** In a cluster of thousands of cheap servers, disks fail or disconnect constantly. If the Master 
  wrote down "Chunk A is on Server 1," that info would quickly become wrong (stale).
* **The Solution:** The **ChunkServer** is the "ultimate source of truth."
  * **On Startup:** The Master starts with an empty location map. It asks all servers: *"What do you have?"*
  * **Ongoing:** It keeps the map fresh via regular **Heartbeat messages**.



---

### 4. Persistence & Recovery (How to survive a crash)

Since RAM is volatile, the Master needs a way to recover its state if power is lost.

#### A. The Operation Log ( The Journal)

* Every change to the metadata (e.g., "Create File X") is written to an **Operation Log** on the Master's local disk.
* **Replication:** Crucially, this log is replicated to remote machines. The operation is not considered "done" until 
  the log is safe on disk locally and remotely.

#### B. Checkpointing (The Snapshot)

Replaying a log with millions of entries takes too long. To speed up recovery:

* **The Mechanism:** The Master periodically saves its entire memory state to disk as a **Checkpoint**.
* **The Format:** It uses a **compact B-tree** format that can be mapped directly into memory without parsing.
* **The "Non-Blocking" Trick:** To avoid pausing the system while saving 10GB of memory, the Master switches to a new 
   log file and creates the checkpoint in a **separate background thread**.

#### C. Recovery Process

When the Master restarts:

1. Loads the latest **Checkpoint** (Instant B-Tree map).
2. Replays the **Operation Log** (only the recent events since the checkpoint).
3. Asks ChunkServers for their inventory (to rebuild the Location Map).


---

## Master Operations

The Master is not just a database of metadata; it is an active manager. It mainly performs the following:

1. Namespace Management & Locking
2. Replica Management
3. Stale replica detection

### 1. Namespace Management & Locking

#### 1. The Data Structure: Hash Map vs. Inodes

In traditional file systems (like Linux ext4), files are stored in a hierarchical tree of **inodes**. To find a file, the OS must traverse the tree, locking directory inodes one by one.

**GFS does NOT use inodes.**
Instead, GFS stores the entire namespace as a massive **Lookup Table (Hash Map)** in RAM.

* **Key:** The full pathname string (e.g., `/usr/bin/java`).
* **Value:** The metadata (Chunk handles, permissions, etc.).

**Implication:**
Since there are no "directory inodes" to act as choke points, GFS implements a sophisticated locking scheme 
using **Read/Write Locks** on the path strings themselves.

---

#### 2. The Locking Strategy (Reader-Writer Locks)

Every node in the path (directories and files) has an associated lock. 
The Master acquires these locks before executing any operation.

* **Read Lock (Shared):** *"I am traversing this or reading this. Please do not delete or rename it 
  while I am here."* Multiple operations can hold a Read Lock on the same node simultaneously.
* **Write Lock (Exclusive):** *"I am modifying this specific node. Everyone else 
  stay away."* Only one operation can hold a Write Lock at a time.

---

#### 3. The Locking Rule (Path Traversal)

To perform an operation on a target file (e.g., `/d1/d2/leaf`), the Master must acquire locks in a specific top-down sequence:

1. **Ancestors (Parents):** Acquire **Read Locks** on all parent directories.
* Read Lock on `/d1`
* Read Lock on `/d1/d2`


2. **Target (Leaf):** Acquire the specific lock needed for the operation.
* **Read Lock** on `/d1/d2/leaf` (if reading).
* **Write Lock** on `/d1/d2/leaf` (if writing/appending).

---

#### 4. Detailed Examples

##### Example A: High Concurrency File Creation

*Scenario:* Two clients want to create files in the same directory at the exact same time.

* **Client A:** Create `/home/user/foo.txt`
* **Client B:** Create `/home/user/bar.txt`

**In a Traditional File System:**
Both clients need a **Write Lock** on the parent directory (`/home/user`) to modify its list of files. 
One must wait for the other. **(Serialized/Slow)**.

**In GFS:**

1. **Client A locks:**
   * Read Lock: `/home`
   * Read Lock: `/home/user` (Shared! Does not block B).
   * **Write Lock:** `/home/user/foo.txt`


2. **Client B locks:**
   * Read Lock: `/home`
   * Read Lock: `/home/user` (Shared! Succeeds immediately).
   * **Write Lock:** `/home/user/bar.txt`

**Result:** Both operations execute **in parallel**. The Read Lock on `/home/user` is sufficient to prevent 
someone from deleting the directory, but it allows new "leaves" to be added concurrently.

---

##### Example B: Snapshotting (The Conflict)

*Scenario:* Client A is writing to `/home/user/foo.txt`. Client B tries to Snapshot `/home/user`.

1. **Client A (Write):** Holds Read Lock on `/home/user` + Write Lock on `foo.txt`.
2. **Client B (Snapshot):**
   * Snapshotting a directory requires a **Read Lock** on the tree to ensure it doesn't change structure.
   * However, if the snapshot involves `foo.txt`, it might conflict depending on the depth of the lock.
   * Crucially, if Client B tried to **Delete/Rename** `/home/user` (which requires a **Write Lock** on `/home/user`), it 
  would fail immediately because Client A holds a Read Lock on it.


---

#### 5. Deadlock Prevention

Since operations require multiple locks, there is a risk of Deadlock (e.g., Process A holds X and wants Y; Process B 
holds Y and wants X). GFS prevents this by enforcing a **Strict Global Ordering** for acquiring locks.

**The Order Rules:**

1. **Level Order (Vertical):** Always lock from Root down to Leaf. (Never lock `/a/b` before `/a`).
2. **Lexicographical Order (Horizontal):** If you need locks on two nodes at the same level, lock them Alphabetically.

**Example Scenario:**
An operation needs to rename `/home/user/b` to `/home/user/a`. It needs locks on both filenames.

* **Wrong Way:** Lock `b` then `a`. (Might deadlock with someone doing `a` -> `b`).
* **Right Way (GFS):**
   1. Lock `/home` (Level 1)
   2. Lock `/home/user` (Level 2)
   3. Lock `/home/user/a` (Level 3, Alphabetical First)
   4. Lock `/home/user/b` (Level 3, Alphabetical Second)


This consistent ordering mathematically guarantees that a cycle (deadlock) cannot occur.


**Interview Win:** Note that file creation does *not* require a write lock on the parent directory. This allows 
multiple files to be created in the same directory simultaneously (high concurrency).

---

Here is a detailed, interview-ready elaboration of **Replica Management**. This is one of the most popular system design topics because it demonstrates how GFS balances *safety* (redundancy) against *performance* (network bandwidth).

---

### 2 Replica Management

The Master is responsible for the entire lifecycle of a chunk replica: **Placement** (Birth), **Re-replication** (Healing), and **Rebalancing** (Optimization).

#### 1. Replica Placement (The "Rack-Aware" Strategy)

When a client creates a new chunk, the Master must decide where to put the 3 copies. It doesn't pick randomly; it uses a specific algorithm to survive hardware failures.

**The Strategy:**

* **Replica 1:** Placed on the **Client's machine** (if the client is a ChunkServer) or a random server in the 
  client's rack. *Why? Maximize write speed for the first copy.*
* **Replica 2:** Placed on a **Different Rack**. *Why? Safety against Rack Failure.*
* **Replica 3:** Placed on a **Different Server** in the **Same Rack** as Replica 2.

**The Trade-Off (Crucial Interview Point):**

* **The Pro (Reads & Safety):**
* **Availability:** If a "Top-of-Rack" switch fails and takes down Rack A, copies still exist on Rack B.
* **Read Bandwidth:** Large reads can be split. A client can pull Byte 0-100 from Rack A and Byte 101-200 
  from Rack B simultaneously, utilizing the aggregate bandwidth of the cluster switches.


* **The Con (Writes):**
* Writing is **Slower**. Data *must* cross network switches to reach different racks. This adds latency compared 
  to writing everything to one rack. GFS accepts this cost for the sake of reliability.



---

#### 2. Replica Creation (Choosing the "Best" Server)

When creating a brand new chunk, the Master filters servers based on three criteria to prevent hotspots.

**Criterion A: Disk Utilization**

* **Goal:** Equalize storage.
* *Example:* If Server A is 80% full and Server B is 20% full, the Master picks Server B.

**Criterion B: Rack Spread**

* **Goal:** Don't put all eggs in one basket.
* *Example:* If Rack 1 already holds 2 replicas of this chunk, the Master forces the 3rd replica to go to Rack 2 or 3.

**Criterion C: Limit "Recent Creations" (The Hidden Trap)**

* **The Insight:** A newly created chunk is usually "Hot." Clients immediately start flooding it with write data.
* **The Risk:** If we place 50 new empty chunks on Server A instantly, Server A's network card will crash under the 
  incoming write traffic 1 second later.
* **The Fix:** The Master tracks how many chunks were created on a server in the last hour. If Server A 
  has "High Recent Creations," the Master skips it, even if it has plenty of empty disk space.

---

#### 3. Re-Replication (The "Healing" Priority Queue)

When a disk dies or a file becomes corrupted, the replication count drops (e.g., from 3 to 2). The Master must clone 
a good copy to restore the count. However, cloning consumes bandwidth, so it must be prioritized.

**The Priority Factors:**

1. **Scarcity (The "Emergency" Level):**
* **Priority High:** A chunk with **1 remaining replica**. (If this drive dies, data is lost forever).
* **Priority Low:** A chunk with **2 remaining replicas**. (Still safe, can wait).


2. **Liveness (User vs. Trash):**
* **Priority High:** Chunks belonging to "Live" files (users are actively reading them).
* **Priority Low:** Chunks belonging to "Deleted" files (sitting in the `.trash` folder waiting for cleanup).



**Throttling:**
The Master limits the number of active cloning jobs per server (e.g., only 2 active clones at a time). This ensures 
that background healing traffic doesn't choke the network for active client reads.

---

#### 4. Replica Rebalancing (The "Optimization" Sweep)

Even if nothing breaks, the Master periodically moves data around to keep the cluster healthy.

**Scenario A: Disk Usage Imbalance**

* *Situation:* After a large deletion, Server A is 10% full while Server B is 90% full.
* *Action:* The Master instructs Server B to clone some chunks to Server A, then delete the originals on B.

**Scenario B: The "New Server" Problem**

* *Situation:* You add a new empty server (Server Z) to the cluster.
* *The Wrong Way:* If the Master immediately sees "0% usage" and fills it with 10,000 chunks, Server Z will be 
  overwhelmed by write traffic (see "Recent Creations" above).
* *The GFS Way:* The Master fills the new server **gradually**. It assigns a few chunks, waits, assigns a few more. 
  This slowly ramps up the traffic to the new machine.


---

### 3. Stale Replica Detection

#### 1. The Core Concept: Version Numbers

In GFS, timestamps are not trusted (clocks drift). Instead, GFS uses **Chunk Version Numbers** to track the "freshness" of data.

* **What is it?** A simple integer associated with every chunk (e.g., `Chunk 555: Version 10`).
* **Where is it stored?**
1. **Master:** In its in-memory map.
2. **ChunkServer:** Persisted to disk alongside the chunk data.



#### 2. The Trigger: Granting a Lease

The Master does not increment the version number for every single *write* (that would be too much metadata traffic). Instead, it increments it when a **Lease** is granted.

* *Logic:* "I am granting a lease to Server A to be the Primary. This marks the beginning of a new 'Generation' of mutations. Anyone who is not part of this generation is now stale."

---

#### 3. Detailed Example Scenario (Step-by-Step)

Let's walk through a failure scenario to see how the system reacts.

**Step 1: The Healthy State**

* **Chunk A** exists on three servers: **S1, S2, S3**.
* **Current Version:** `v1`.
* **State:** All three servers store `v1` on their local disks. The Master knows the current version is `v1`.

**Step 2: The Failure**

* **Server S3 crashes.** It goes offline.

**Step 3: The Mutation (The Divergence)**

* A client wants to append data to Chunk A.

* The Master notices S3 is down. It decides to proceed with only **S1 and S2**.

* **Action:**
   1. Master increments Chunk A's version: `v1`  `v2`.
   2. Master grants a Lease to S1 (Primary).
   3. Master instructs S1 and S2: *"Update your local version to v2."*
   4. S1 and S2 write `v2` to disk and accept the new data.

* **Current State:**
  * Master: `v2`
  * S1, S2: `v2` (Fresh)
  * S3 (Offline): `v1` (Stale)



**Step 4: The Restart (The Trap)**

* Server S3 reboots and rejoins the cluster.
* **The Handshake:** S3 sends a report to the Master: *"I have Chunk A, Version 1."*

**Step 5: The Verdict**

* The Master compares the reported version (`v1`) with its known version (`v2`).
* **Detection:** `v1 < v2`.
* **Action:** The Master marks S3's copy of Chunk A as **Stale**.
  * It will **not** send S3's location to any clients reading Chunk A.
  * It effectively considers the chunk non-existent on S3.



**Step 6: Cleanup**

* During the regular Garbage Collection sweep, the Master tells S3: *"Your copy of Chunk A is garbage. Delete it."*

---

#### 4. The Edge Case: Client Caching

There is a small window where a client *might* read stale data.

**The Scenario:**

1. **Cache Hit:** A Client asked the Master for Chunk A's location *before* S3 crashed. The Client cached: `Chunk A is on S3`.
2. **Crash & Update:** S3 crashes, S1/S2 update to `v2`, S3 comes back up with `v1`.
3. **The Read:** The Client (relying on its cache) tries to read from S3 immediately after it restarts.
4. **The Result:** S3 serves the read request because S3 *thinks* it is valid (it doesn't know about `v2` yet).

**Why this isn't a disaster (The Append-Only Safety):**
In many file systems, this would return corrupted data. In GFS, it usually just results in a **"Premature End of File"**.

* *Reason:* GFS files are mostly logs.
* *Fresh Copy (S1):* Contains `Record 1, Record 2, Record 3`.
* *Stale Copy (S3):* Contains `Record 1, Record 2`.
* If the client reads from S3, it doesn't see "Wrong Data" for Record 3; it just sees **nothing**. 
  It thinks the file ends at Record 2.
* *Impact:* The client might miss the latest log entry, but it won't get garbage data. 
  When the client retries (or cache expires), it will switch to S1 and see Record 3.

---

## Anatomy of a Read Operation

The goal of the read operation is to get the Master out of the way as quickly as possible so the 
client can stream data directly from the servers.

### Step 1: Local Translation (Client Side Math)

Before the client sends any network packet, it does some local math.

* **Input:** The Application says *"Read byte 100,000,000 (100MB) from `foo.txt`."*
* **Calculation:** Since the chunk size is fixed at **64 MB**, the GFS client library calculates:
* **Chunk Index:** `floor(100MB / 64MB)` = **Index 1** (The 2nd chunk).
* **Byte Offset:** `100MB % 64MB` = **36 MB** (Start reading at the 36MB mark inside that chunk).



### Step 2: Metadata Request (The Control Flow)

The client sends a lightweight RPC to the Master.

* **Request:** *"Who has `foo.txt`, Chunk Index 1?"*
* **Note:** The client does *not* ask for the actual data, just the location.

### Step 3: Master Reply with "Lookahead"

The Master looks up its RAM table and replies.

* **Reply:** *"Chunk Index 1 corresponds to **Handle #555**. It is located on **Server A, Server B, Server C**."*
* **The Optimization (Lookahead):** The Master guesses that if you are reading Chunk 1, you will probably read Chunk 2 next.
* Instead of just sending info for Chunk 1, it **piggybacks** the metadata for **Chunk 2 and Chunk 3**.
* *Benefit:* This saves the client from having to talk to the Master again in a few seconds.



### Step 4: Caching (The Memory)

The client saves this data in its local **Client Cache**.

* **Key:** `foo.txt` + `Chunk 1`.
* **Value:** `Handle #555` + `[Server A, Server B, Server C]`.
* *Benefit:* Future reads to this range happen with **Zero** Master interaction until the cache expires.

### Step 5: The "Closest" Replica (The Data Flow)

The client needs to fetch the actual bytes. It looks at the list of 3 replicas (A, B, C) and picks the "closest" one.

* **Definition of "Closest":**
1. **Same Machine:** (Fastest) If the client is running on Server A, read from local disk.
2. **Same Rack:** (Fast) If Server B is in the same rack, read from B.
3. **Different Rack:** (Slow) Server C is across the switch. Avoid if possible.


* **Action:** Client sends a request to **Server B** (Same Rack).
* **Request:** *"Read Chunk #555, Range 36MB - 40MB."*



### Step 6: Data Transfer

The ChunkServer (Server B) reads the file from its local Linux file system, verifies the **Checksum**
and streams the bytes directly to the client.

---

### Summary of the "Separation of Flows"

| Flow Type        | Who is involved?    | What is transferred?          | Size         |
|------------------|---------------------|-------------------------------|--------------|
| **Control Flow** | Client  Master      | Metadata (Locations, Handles) | Tiny (Bytes) |
| **Data Flow**    | Client  ChunkServer | File Content                  | Huge (MB/GB) |

**Why this matters for the Interview:**

If the Master handled the data (like a proxy), its network card would be saturated instantly. 
By moving the data flow to the edges (ChunkServers), the system's total throughput scales linearly with the number of machines.


## Section 4: Storage Efficiency & Optimizations

This section covers how GFS solves the two biggest downsides of using huge 64MB chunks: **Wasted Space** and **Traffic Jams**.

### 1. Lazy Space Allocation (The "Expandable Folder")

**The Problem:**
If you force every file to take up 64MB blocks, a 1KB text file would waste **63.99 MB** of disk space. 
This is called **Internal Fragmentation**.

**The Solution:**
GFS chunks are stored as **regular Linux files**.

* **Lazy Growth:** When a chunk is created, the ChunkServer does *not* reserve 64MB of physical disk space. 
  It creates a 0-byte file.
* **Extension:** As the client appends data, the Linux file grows naturally. It only occupies the physical bytes 
  it actually needs.
* **Result:** A 1KB file takes up 1KB of disk space (plus a tiny bit of metadata), not 64MB.

---

### 2. The "Small File" Problem (Hotspots)

**The Problem:**
Large chunks are great for spreading out the load of *large* files (10GB file = 160 chunks = 160 servers working in parallel).
But for a **Small File** (e.g., a 1MB configuration file or executable):

* It fits entirely inside **1 Chunk**.
* That 1 Chunk lives on only **3 Servers**.
* **The Hotspot:** If 10,000 clients try to read that file at once (e.g., a huge job starts up), they all hammer those
  same 3 servers. The rest of the cluster sits idle.

**The Solution:**
GFS uses two tricks to mitigate this:

1. **Higher Replication:** The Master spots these "Hot" files and increases their replication 
  factor (e.g., from 3 copies to 20 copies). This spreads the read load across more machines.
2. **Staggered Access (Random Delays):** Clients are programmed to wait a random amount of time before 
  reading such files. This smooths out the "thundering herd" of traffic.

---

## Section 5: Anatomy of Operations (Deep Dive)

This section details the precise mechanics of how data moves. The critical design principle here 
is the **Separation of Control Plane (Metadata) from Data Plane (Bytes)**.

### 1. Anatomy of a Read Operation

*Goal: Minimize Master involvement to prevent bottlenecks.*

**Step 1: Client Calculation (No Network)**
The client application requests a specific byte range (e.g., "Read byte 100MB to 105MB from `foo.txt`").

* The GFS Client Library translates this locally. Since chunks are fixed at **64MB**, it calculates:
  * `100MB` falls into **Chunk Index 1** (The second chunk).
  * The **Offset** inside that chunk is `100MB - 64MB = 36MB`.



**Step 2: Metadata Request (Control Plane)**
The client sends an RPC to the Master: *"Where is `foo.txt`, Chunk Index 1?"*

**Step 3: Master Reply with "Lookahead"**
The Master replies with:

  1. The **Chunk Handle** (e.g., ID #555).
  2. The **Replica Locations** (e.g., Server A, Server B, Server C).
  3. **Optimization:** The Master *also* sends metadata for the **next few chunks** (Index 2, 3, etc.). This avoids 
    future round-trips since most reads are sequential.

**Step 4: Caching**
The client caches this mapping (File + Index  Handle + Locations). It will now read the entire chunk without speaking to the Master again.

**Step 5: Data Transfer (Data Plane)**
The client picks the **closest** replica (network-wise) and sends the read 
request: *"Read Chunk #555, Range 36MB-41MB."* The ChunkServer streams the data directly to the client.

---

### 2. Anatomy of a Write Operation

The Write operation is complex because it requires locking and ordering. GFS uses a specific **"Data Pipelining"** 
strategy to maximize bandwidth.

**Phase 1: Lease & Discovery**

1. **Ask Master:** Client asks Master: *"Who holds the lease for Chunk #555?"*
2. **Grant Lease:** If no one has it, the Master grants a **60-second lease** to one replica (making it the **Primary**).
3. **Reply:** Master returns the identity of the **Primary** and the **Secondaries**.

**Phase 2: The Data Flow (The Pipeline)**

*Goal: Utilize the upload bandwidth of every machine, not just the client.*

4. **Push Data:** The client pushes the data to the **closest** replica (e.g., Server B). It does *not* matter if B is the Primary or not.
5. **Daisy Chain:** Server B forwards the data to the next closest replica (Server A), which forwards to the last one (Server C).
    * *Why?* If the Client had to send to all 3 servers, its network card would be the bottleneck (1x speed). 
     By chaining, the Client sends once, and the servers help propagate (3x aggregate speed).


6. **Buffer:** The data sits in the **LRU Buffer Cache (RAM)** of all 3 replicas. It is **not** written to the file yet.

**Phase 3: The Control Flow (The Trigger)**

*Goal: Decide the official order of writes.*

7. **Write Request:** Once all replicas acknowledge they have the data buffered, the Client sends a "Write" command to the **Primary**.
8. **Serialization:** The Primary assigns a **Serial Number** to the write (e.g., *"This is Mutation #85"*). 
  This number defines the official global order.
9. **Forwarding:** The Primary tells all Secondaries: *"Commit the buffered data as Mutation #85."*
10. **Execution:** All replicas write the data from RAM to Disk in that exact serial order.
11. **Reply:** The Primary reports "Success" to the Client.

---

### 3. Anatomy of a Record Append (The "Atomic" Special)

This is the unique feature of GFS. Standard writes specify an **offset** (e.g., "Write at byte 100"), which requires 
locking to prevent overwrites. **Record Append** does not.

**The "At-Least-Once" Logic:**

1. **Request:** The client pushes data to all replicas (just like a write) and tells the Primary: *"Append this."*
2. **Size Check (Primary Only):** The Primary checks the current chunk.
    * **If data fits:** Proceed to Step 3.
    * **If data doesn't fit (Crosses 64MB boundary):**
        * The Primary **pads** the current chunk to the full 64MB limit with empty bytes.
        * It commands secondaries to do the same.
        * It tells the Client: *"Chunk is full. Please retry on the next chunk."* (This ensures records are never split across chunks).

3. **The Write:** If it fits, the Primary appends the data to its local file, calculates the offset, and tells
  secondaries to write at that **exact same offset**.
4. **Failure & Retry:**
    * If the append fails at any secondary (e.g., disk error), the client **retries**.
    * **Result:** The Primary might have appended it successfully (Attempt 1). On retry, it appends it *again* (Attempt 2).
    * **Consistency:** The file ends up with **Duplicate Records**, but the data is guaranteed to be there *at least once*.

---

### 4. GFS Consistency Model

It is critical to distinguish between "Undefined" and "Inconsistent".

| Scenario                       | State                        | Description                                                                                                            |
|--------------------------------|------------------------------|------------------------------------------------------------------------------------------------------------------------|
| **Success (Serial Write)**     | **Defined & Consistent**     | All replicas are identical. The client sees exactly what they wrote.                                                   |
| **Concurrent Write (Overlap)** | **Consistent but Undefined** | All replicas are identical (consistent), but the data is a jumbled mix of fragments from multiple clients (undefined). |
| **Record Append (Success)**    | **Defined & Consistent**     | All replicas are identical. The record is atomic (no fragmentation).                                                   |
| **Record Append (Failure)**    | **Inconsistent**             | Replicas might differ (some have the record, some don't). GFS hides this by forcing a retry, leading to duplicates.    |

---

**Ready for Section 6: Fault Tolerance & High Availability?** (This covers Checksums, Shadow Masters, and Recovery).

Here is **Section 6: Fault Tolerance & High Availability**.

In GFS, failure is not an exception; it is the norm. The system is designed to keep running even when 
components (Master, ChunkServers, Disks) are broken.

---

## Section 6: Fault Tolerance & High Availability

### 1. High Availability (Keeping the lights on)

GFS uses two simple but powerful strategies to stay online: **Replication** and **Fast Recovery**.

#### A. Chunk Replication

* **The Rule:** Every chunk is replicated to multiple servers (Default: **3**).
* **The Benefit:** If a server crashes or a disk dies, the data is still available on 2 other machines.
* **The Master's Job:** It constantly monitors replicas. If the count drops below 3, it clones a surviving replica to a new server.

#### B. Fast Recovery

* **Master Restart:** If the Master process crashes, it is designed to restart in seconds. Since the entire 
  state is in memory (loaded from the fast Checkpoint), it comes back online almost instantly.
* **Client Retry:** Clients and ChunkServers are designed to simply "retry" their requests if the Master is momentarily down.

---

### 2. Master Failure (The "Brain" Death)

The Master is a **Single Point of Failure**. If the machine physically burns down, GFS needs a plan.

#### A. The Operation Log (The Backup Brain)

* The Master's "Operation Log" is the only thing that matters.
* **Replication:** This log is replicated to multiple **remote machines** in real-time.
* **Failover:** If the primary Master machine dies, a monitoring system detects it (missing heartbeats) and 
  starts a **New Master** process on one of the remote machines that has the log.

#### B. Shadow Masters (Read-Only Mode)

To improve read availability, GFS can run **Shadow Masters**.

* **Role:** They are "Read-Only" replicas of the Master.
* **Sync:** They lag slightly behind the Primary by tailing the Operation Log.
* **Use Case:** If the Primary Master is down (or busy), clients can still read metadata from Shadow Masters. 
  They might get slightly stale info, but for many apps, that is better than no info.

---

### 3. 3. Replica Failure & Stale Data Detection

When a ChunkServer crashes and misses updates, its data becomes "Stale." The Master must detect this to 
prevent clients from reading old data.

**Detection Mechanism: Version Numbers**

The Detection Mechanism (Version Numbers):

1. **The Mutation:** Whenever the Master grants a lease for a write, it increments the Chunk Version Number (e.g., v10 → v11).
2. **The Update:** It commands all live replicas to update their stored version to v11.
3. **The Failure:** Suppose Server C is offline during this write. It remains at v10.
4. **The Restart:** When Server C restarts, it reports its inventory to the Master: "I have Chunk X, Version 10."
5. **The Verdict:** The Master sees that the current system version is v11. It instantly marks Server C's copy 
  as "Stale" and will not send its location to clients.

---

### 4. Data Integrity (Checksums)

Hard drives are notorious for "Silent Corruption" (flipping a bit without telling anyone). Since GFS stores 
PetaBytes of data, this happens often.

**The Solution: Checksumming**

* **The Block:** Every 64MB chunk is divided into tiny **64KB blocks**.
* **The Checksum:** Each 64KB block has a 32-bit Checksum stored in RAM/Log.

**How it protects Reads:**

1. **Verify First:** Before a ChunkServer sends data to a client, it checks the checksum.
2. **Corruption Detected:** If the checksum mismatches:
* It returns an **Error** to the client.
* It reports the corruption to the **Master**.


3. **Healing:** The client reads from a different replica. The Master clones a good replica to replace the bad one.

**Optimization (Why it's fast):**

* **Reads:** Checksums are only verified for the specific range being read.
* **Appends:** GFS doesn't verify the whole chunk. It just incrementally updates the checksum for the last block and 
  adds new checksums for the new blocks.

---

## Section 7: Garbage Collection & Maintenance.

### 1. Lazy Deletion (The "Trash Bin" Strategy)

#### Why do we need this?

1. **Safety (Undo):** In a system this large, accidental deletions are common. Immediate physical deletion would be 
  irreversible. GFS provides a safety buffer.
2. **Performance:** Reclaiming physical storage requires writing to disk (updating bitmaps/inodes). Doing this 
  synchronously while a client waits is slow. GFS batches this work during idle times.

#### The Workflow (Step-by-Step):

**Step 1: The Logical Delete (Rename)**
When a client issues a command `delete("data.log")`:

* The Master acts immediately, but it does **not** delete the file.
* It **Renames** the file to a hidden name that includes the deletion timestamp (e.g., `.trash/data.log_20260206`).
* **Result:** The file disappears from the normal namespace, but it still exists.
* *Read:* You can still read it if you know the hidden path.
* *Recover:* You can "undelete" it by renaming it back to the original name.



**Step 2: The Grace Period**
The file sits in the hidden `.trash` directory for a configurable interval (default: **3 days**).

**Step 3: The Metadata Sweep**
The Master performs a periodic background scan of its namespace (RAM).

* It identifies files in `.trash` that are older than 3 days.
* It removes the **Metadata** for these files from its RAM.
* **Crucial Point:** The Master *still* hasn't touched the physical disks on the ChunkServers. It has simply
  "forgotten" that these files exist. This effectively severs the link between the filename and the chunk handles.

---

### 2. Orphaned Chunk Collection (Physical Deletion)

Now that the Master has "forgotten" the chunks, they have become **Orphans**—chunks that exist on disk but have no 
file metadata pointing to them. The system must find and kill them.

#### The Workflow (The Heartbeat Handshake):

**Step 1: The Chunk Report**
During the regular **Heartbeat** exchange, every ChunkServer sends the Master a list of all the chunks it currently holds on its disk.

**Step 2: The Comparison**
The Master checks this list against its own **In-Memory Metadata Map**.

* *Chunk A:* "I know this one. It belongs to `user_profile.db`. Keep it."
* *Chunk B:* "I have no record of this ID." (This is likely a chunk that was part of the file deleted in the previous step).

**Step 3: The Death Sentence**
The Master replies to the ChunkServer: *"I don't know the following IDs: [Chunk B]. You are free to delete them."*

**Step 4: The Execution**
The ChunkServer receives the command and deletes the actual `.chk` files from its local Linux file system. 
This is when the disk space is finally freed.

---


