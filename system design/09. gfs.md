<!-- TOC -->
* [Design Google File System (GFS)](#design-google-file-system-gfs)
  * [Resources](#resources)
  * [Section 1: The Problem & The Architecture](#section-1-the-problem--the-architecture)
    * [1. The Design Goals (Why GFS exists)](#1-the-design-goals-why-gfs-exists)
    * [2. The Architecture Components](#2-the-architecture-components)
      * [A. The Chunk (The Unit of Storage)](#a-the-chunk-the-unit-of-storage)
      * [B. The Three Actors](#b-the-three-actors)
    * [3. The "Single Master" Controversy](#3-the-single-master-controversy)
  * [Section 2: Metadata Management & Persistence](#section-2-metadata-management--persistence)
    * [1. What Metadata does the Master store?](#1-what-metadata-does-the-master-store)
    * [2. The "In Memory" Strategy](#2-the-in-memory-strategy)
    * [3. The "Inversion of Authority" (Why Chunk Locations aren't saved)](#3-the-inversion-of-authority-why-chunk-locations-arent-saved)
    * [4. Persistence & Recovery (How to survive a crash)](#4-persistence--recovery-how-to-survive-a-crash)
      * [A. The Operation Log ( The Journal)](#a-the-operation-log--the-journal)
      * [B. Checkpointing (The Snapshot)](#b-checkpointing-the-snapshot)
      * [C. Recovery Process](#c-recovery-process)
  * [Section 3: Master Operations](#section-3-master-operations)
    * [1. Namespace Management & Locking](#1-namespace-management--locking)
    * [2. Replica Placement (The "Birth" Strategy)](#2-replica-placement-the-birth-strategy)
    * [3. Re-Replication (The "Healing" Strategy)](#3-re-replication-the-healing-strategy)
    * [4. Rebalancing (The "Optimization" Strategy)](#4-rebalancing-the-optimization-strategy)
  * [Section 4: Storage Efficiency & Optimizations](#section-4-storage-efficiency--optimizations)
    * [1. Lazy Space Allocation (The "Expandable Folder")](#1-lazy-space-allocation-the-expandable-folder)
    * [2. The "Small File" Problem (Hotspots)](#2-the-small-file-problem-hotspots)
  * [Section 5: Anatomy of Operations (Deep Dive)](#section-5-anatomy-of-operations-deep-dive)
    * [1. Anatomy of a Read Operation](#1-anatomy-of-a-read-operation)
    * [2. Anatomy of a Write Operation](#2-anatomy-of-a-write-operation)
    * [3. Anatomy of a Record Append (The "Atomic" Special)](#3-anatomy-of-a-record-append-the-atomic-special)
    * [4. GFS Consistency Model](#4-gfs-consistency-model)
  * [Section 6: Fault Tolerance & High Availability](#section-6-fault-tolerance--high-availability)
    * [1. High Availability (Keeping the lights on)](#1-high-availability-keeping-the-lights-on)
      * [A. Chunk Replication](#a-chunk-replication)
      * [B. Fast Recovery](#b-fast-recovery)
    * [2. Master Failure (The "Brain" Death)](#2-master-failure-the-brain-death)
      * [A. The Operation Log (The Backup Brain)](#a-the-operation-log-the-backup-brain)
      * [B. Shadow Masters (Read-Only Mode)](#b-shadow-masters-read-only-mode)
    * [3. 3. Replica Failure & Stale Data Detection](#3-3-replica-failure--stale-data-detection)
    * [4. Data Integrity (Checksums)](#4-data-integrity-checksums)
  * [Section 7: Garbage Collection & Maintenance.](#section-7-garbage-collection--maintenance)
    * [1. Lazy Deletion (The "Trash Bin" Strategy)](#1-lazy-deletion-the-trash-bin-strategy)
      * [Why do we need this?](#why-do-we-need-this)
      * [The Workflow (Step-by-Step):](#the-workflow-step-by-step)
    * [2. Orphaned Chunk Collection (Physical Deletion)](#2-orphaned-chunk-collection-physical-deletion)
      * [The Workflow (The Heartbeat Handshake):](#the-workflow-the-heartbeat-handshake)
<!-- TOC -->

# Design Google File System (GFS)

## Resources

* [Google File System - Paper that inspired Hadoop](https://www.youtube.com/watch?v=eRgFNW4QFDc)

---

This is a comprehensive, interview-ready consolidation of the Google File System (GFS). 
I have restructured the information to build logically: starting with **The Why (Goals)**, 
moving to **The Architecture (Components)**, and then diving into the complex **Mechanisms**.

Here is **Section 1: The Foundation**.

---

## Section 1: The Problem & The Architecture

### 1. The Design Goals (Why GFS exists)

GFS was not built to be a "better" general-purpose file system. It was built to solve a specific Google problem in the early 2000s.

* **Problem A: Hardware Fails Constantly.**
    * Google used thousands of cheap, "commodity" servers.
    * *Assumption:* At any moment, some hard drive, power supply, or network switch is broken. The software **must** handle this automatically.


* **Problem B: Files are Huge.**
    * Web crawls are multi-gigabyte or terabyte files.
    * Standard file systems (4KB blocks) crumble under this weight (too much metadata).


* **Problem C: Write Once, Read Many.**
    * Data is rarely overwritten. It is usually appended to (logs) or read sequentially (scanning the web).
    * *Decision:* Optimize for **Append** and **Sequential Read**. Random write is supported but not optimized.



---

### 2. The Architecture Components

GFS uses a **Master-Slave** architecture.

#### A. The Chunk (The Unit of Storage)

Instead of small blocks (4KB), GFS uses massive blocks.

* **Size:** **64 MB**.
* **Identification:** Each chunk has a unique 64-bit **Chunk Handle**.
* **Storage:** Chunks are stored as regular Linux files on the servers.
* **Why 64 MB?** (Key Interview Question)
  1. **Reduces Metadata:** Fewer chunks mean the Master can keep *all* metadata in RAM.
  2. **Reduces Network Overhead:** Clients can keep a persistent TCP connection to a server for longer.
  3. **Reduces Chatter:** Clients ask the Master for location once, then read 64MB of data without bothering the Master again.



#### B. The Three Actors

| Component           | Role                                                                                                           | State Storage                                                       |
|---------------------|----------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------|
| **1. The Master**   | **The Coordinator.** Stores metadata (file names, mapping). Does NOT store data.                               | **RAM** (Fast access). Persisted to disk via **Operation Log**.     |
| **2. Chunkservers** | **The Workers.** Store the actual 64MB chunks. Read/Write bits.                                                | **Linux Disk.**                                                     |
| **3. The Client**   | **The Library.** Linked into apps. Talks to Master for metadata, then talks directly to Chunkservers for data. | **RAM Cache.** Caches metadata (where chunks are) for a short time. |

---

### 3. The "Single Master" Controversy

**Question:** *Isn't a single master a bottleneck?*

**Answer:** It *would* be, if the data flowed through it.

* **The Fix:** GFS separates **Control Flow** from **Data Flow**.
  * The Master only sends tiny metadata packets (e.g., "Chunk 1 is on Server A").
  * The heavy data (Gigabytes) flows directly from Server  Client.
  * This allows one Master to coordinate thousands of servers without getting overwhelmed.


---

## Section 2: Metadata Management & Persistence

### 1. What Metadata does the Master store?

The Master tracks three specific types of data. It is crucial to know which ones are saved to disk and which are not.

| Metadata Type                | Description                                                           | Stored on Disk?      |
|------------------------------|-----------------------------------------------------------------------|----------------------|
| **1. Namespace**             | The directory tree (e.g., `/home/user/foo.txt`).                      | **YES** (Persistent) |
| **2. File-to-Chunk Mapping** | Which chunks belong to which file (e.g., `foo.txt` = Chunk #55, #56). | **YES** (Persistent) |
| **3. Chunk Locations**       | Which servers hold Chunk #55? (e.g., `Server A, Server B`).           | **NO** (Volatile)    |

### 2. The "In Memory" Strategy

The Master keeps **all** metadata in main memory (RAM).

* **Why?** It makes operations like namespace lookup and "Garbage Collection" scans instantaneous.
* **The Limit:** The text notes that system capacity is limited by the Master's RAM. However, since it only takes ~64 bytes of metadata to track a massive 64MB chunk, this is rarely a bottleneck in practice.

---

### 3. The "Inversion of Authority" (Why Chunk Locations aren't saved)

This is a specific design choice highlighted in the text: **"The master does not keep a persistent record of which ChunkServers have a replica."**

* **The Logic:** In a cluster of thousands of cheap servers, disks fail or disconnect constantly. If the Master wrote down "Chunk A is on Server 1," that info would quickly become wrong (stale).
* **The Solution:** The **ChunkServer** is the "ultimate source of truth."
  * **On Startup:** The Master starts with an empty location map. It asks all servers: *"What do you have?"*
  * **Ongoing:** It keeps the map fresh via regular **Heartbeat messages**.



---

### 4. Persistence & Recovery (How to survive a crash)

Since RAM is volatile, the Master needs a way to recover its state if power is lost.

#### A. The Operation Log ( The Journal)

* Every change to the metadata (e.g., "Create File X") is written to an **Operation Log** on the Master's local disk.
* **Replication:** Crucially, this log is replicated to remote machines. The operation is not considered "done" until the log is safe on disk locally and remotely.

#### B. Checkpointing (The Snapshot)

Replaying a log with millions of entries takes too long. To speed up recovery:

* **The Mechanism:** The Master periodically saves its entire memory state to disk as a **Checkpoint**.
* **The Format:** It uses a **compact B-tree** format that can be mapped directly into memory without parsing.
* **The "Non-Blocking" Trick:** To avoid pausing the system while saving 10GB of memory, the Master switches to a new log file and creates the checkpoint in a **separate background thread**.

#### C. Recovery Process

When the Master restarts:

1. Loads the latest **Checkpoint** (Instant B-Tree map).
2. Replays the **Operation Log** (only the recent events since the checkpoint).
3. Asks ChunkServers for their inventory (to rebuild the Location Map).


---

## Section 3: Master Operations

The Master is not just a database of metadata; it is an active manager. It performs two main categories of work: **Namespace Management** (Locking) and **Replica Management** (Placement & Balancing).

### 1. Namespace Management & Locking

**The "No-Inode" Structure:**
Unlike standard file systems (like Linux ext4) that use a tree of inodes, GFS uses a massive **Hash Map**.

* **Key:** Full pathname (e.g., `/home/user/foo.txt`).
* **Value:** Metadata (Chunk handles, etc.).

**The Locking Strategy (No Directory Inodes):**
Since there are no directory nodes to lock, GFS uses **Read/Write locks** on the path names themselves.

* **To Modify a File (`/d1/d2/leaf`):**
  * **Read Locks:** Acquired on all parent directories (`/d1` and `/d1/d2`). This prevents someone from deleting or renaming the parent while you are working inside it.
  * **Write Lock:** Acquired on the target (`/d1/d2/leaf`).


* **Deadlock Prevention:** Locks are always acquired in a consistent order:
  1. First by **Level** in the tree.
  2. Then **Lexicographically** (Alphabetically) within the same level.



**Interview Win:** Note that file creation does *not* require a write lock on the parent directory. This allows multiple files to be created in the same directory simultaneously (high concurrency).

---

### 2. Replica Placement (The "Birth" Strategy)

When creating a new chunk, the Master doesn't pick servers randomly. It optimizes for **Reliability** and **Bandwidth**.

**The Rack-Aware Heuristic:**

* **Goal:** Maximize data availability even if an entire rack fails (e.g., top-of-rack switch failure).
* **Strategy:** Replicas are spread across **different racks**.
* *Trade-off:* Writing to different racks is slower (data crosses switches), but it guarantees that data survives a rack failure. It also allows clients to read from multiple racks, exploiting aggregate bandwidth.



**Server Selection Criteria:**

1. **Disk Utilization:** Pick servers with below-average disk usage.
2. **Recent Creations:** Avoid servers that have recently created many chunks. (New chunks attract heavy write traffic; we don't want to saturation one server's IO).

---

### 3. Re-Replication (The "Healing" Strategy)

When the number of valid replicas drops (e.g., a disk dies), the Master must clone chunks to restore safety.

**Priority Queue (Triage):**
The Master does not fix everything at once. It prioritizes:

1. **Scarcity:** How far is the chunk from its replication goal? (A chunk with **1** replica is fixed before a chunk with **2**).
2. **Liveness:** Is the file "Live" (active user) or "Deleted" (waiting in trash)? Live files get priority.

**Throttling:**
The Master limits the bandwidth used for cloning so that the "healing" traffic doesn't choke the network for active client requests.

---

### 4. Rebalancing (The "Optimization" Strategy)

The Master periodically moves replicas around for two reasons:

1. **Load Balancing:** To even out disk usage across the cluster.
2. **Gradual Filling:** When a **new ChunkServer** joins, the Master fills it *slowly*. It avoids flooding the empty server with writes, which would overwhelm its network card.


---


## Section 4: Storage Efficiency & Optimizations

This section covers how GFS solves the two biggest downsides of using huge 64MB chunks: **Wasted Space** and **Traffic Jams**.

### 1. Lazy Space Allocation (The "Expandable Folder")

**The Problem:**
If you force every file to take up 64MB blocks, a 1KB text file would waste **63.99 MB** of disk space. This is called **Internal Fragmentation**.

**The Solution:**
GFS chunks are stored as **regular Linux files**.

* **Lazy Growth:** When a chunk is created, the ChunkServer does *not* reserve 64MB of physical disk space. It creates a 0-byte file.
* **Extension:** As the client appends data, the Linux file grows naturally. It only occupies the physical bytes it actually needs.
* **Result:** A 1KB file takes up 1KB of disk space (plus a tiny bit of metadata), not 64MB.

---

### 2. The "Small File" Problem (Hotspots)

**The Problem:**
Large chunks are great for spreading out the load of *large* files (10GB file = 160 chunks = 160 servers working in parallel).
But for a **Small File** (e.g., a 1MB configuration file or executable):

* It fits entirely inside **1 Chunk**.
* That 1 Chunk lives on only **3 Servers**.
* **The Hotspot:** If 10,000 clients try to read that file at once (e.g., a huge job starts up), they all hammer those same 3 servers. The rest of the cluster sits idle.

**The Solution:**
GFS uses two tricks to mitigate this:

1. **Higher Replication:** The Master spots these "Hot" files and increases their replication factor (e.g., from 3 copies to 20 copies). This spreads the read load across more machines.
2. **Staggered Access (Random Delays):** Clients are programmed to wait a random amount of time before reading such files. This smooths out the "thundering herd" of traffic.

---

## Section 5: Anatomy of Operations (Deep Dive)

This section details the precise mechanics of how data moves. The critical design principle here is the **Separation of Control Plane (Metadata) from Data Plane (Bytes)**.

### 1. Anatomy of a Read Operation

*Goal: Minimize Master involvement to prevent bottlenecks.*

**Step 1: Client Calculation (No Network)**
The client application requests a specific byte range (e.g., "Read byte 100MB to 105MB from `foo.txt`").

* The GFS Client Library translates this locally. Since chunks are fixed at **64MB**, it calculates:
  * `100MB` falls into **Chunk Index 1** (The second chunk).
  * The **Offset** inside that chunk is `100MB - 64MB = 36MB`.



**Step 2: Metadata Request (Control Plane)**
The client sends an RPC to the Master: *"Where is `foo.txt`, Chunk Index 1?"*

**Step 3: Master Reply with "Lookahead"**
The Master replies with:

  1. The **Chunk Handle** (e.g., ID #555).
  2. The **Replica Locations** (e.g., Server A, Server B, Server C).
  3. **Optimization:** The Master *also* sends metadata for the **next few chunks** (Index 2, 3, etc.). This avoids future round-trips since most reads are sequential.

**Step 4: Caching**
The client caches this mapping (File + Index  Handle + Locations). It will now read the entire chunk without speaking to the Master again.

**Step 5: Data Transfer (Data Plane)**
The client picks the **closest** replica (network-wise) and sends the read request: *"Read Chunk #555, Range 36MB-41MB."* The ChunkServer streams the data directly to the client.

---

### 2. Anatomy of a Write Operation

The Write operation is complex because it requires locking and ordering. GFS uses a specific **"Data Pipelining"** strategy to maximize bandwidth.

**Phase 1: Lease & Discovery**

1. **Ask Master:** Client asks Master: *"Who holds the lease for Chunk #555?"*
2. **Grant Lease:** If no one has it, the Master grants a **60-second lease** to one replica (making it the **Primary**).
3. **Reply:** Master returns the identity of the **Primary** and the **Secondaries**.

**Phase 2: The Data Flow (The Pipeline)**

*Goal: Utilize the upload bandwidth of every machine, not just the client.*

4. **Push Data:** The client pushes the data to the **closest** replica (e.g., Server B). It does *not* matter if B is the Primary or not.
5. **Daisy Chain:** Server B forwards the data to the next closest replica (Server A), which forwards to the last one (Server C).
    * *Why?* If the Client had to send to all 3 servers, its network card would be the bottleneck (1x speed). By chaining, the Client sends once, and the servers help propagate (3x aggregate speed).


6. **Buffer:** The data sits in the **LRU Buffer Cache (RAM)** of all 3 replicas. It is **not** written to the file yet.

**Phase 3: The Control Flow (The Trigger)**

*Goal: Decide the official order of writes.*

7. **Write Request:** Once all replicas acknowledge they have the data buffered, the Client sends a "Write" command to the **Primary**.
8. **Serialization:** The Primary assigns a **Serial Number** to the write (e.g., *"This is Mutation #85"*). This number defines the official global order.
9. **Forwarding:** The Primary tells all Secondaries: *"Commit the buffered data as Mutation #85."*
10. **Execution:** All replicas write the data from RAM to Disk in that exact serial order.
11. **Reply:** The Primary reports "Success" to the Client.

---

### 3. Anatomy of a Record Append (The "Atomic" Special)

This is the unique feature of GFS. Standard writes specify an **offset** (e.g., "Write at byte 100"), which requires locking to prevent overwrites. **Record Append** does not.

**The "At-Least-Once" Logic:**

1. **Request:** The client pushes data to all replicas (just like a write) and tells the Primary: *"Append this."*
2. **Size Check (Primary Only):** The Primary checks the current chunk.
    * **If data fits:** Proceed to Step 3.
    * **If data doesn't fit (Crosses 64MB boundary):**
        * The Primary **pads** the current chunk to the full 64MB limit with empty bytes.
        * It commands secondaries to do the same.
        * It tells the Client: *"Chunk is full. Please retry on the next chunk."* (This ensures records are never split across chunks).

3. **The Write:** If it fits, the Primary appends the data to its local file, calculates the offset, and tells secondaries to write at that **exact same offset**.
4. **Failure & Retry:**
    * If the append fails at any secondary (e.g., disk error), the client **retries**.
    * **Result:** The Primary might have appended it successfully (Attempt 1). On retry, it appends it *again* (Attempt 2).
    * **Consistency:** The file ends up with **Duplicate Records**, but the data is guaranteed to be there *at least once*.

---

### 4. GFS Consistency Model

It is critical to distinguish between "Undefined" and "Inconsistent".

| Scenario                       | State                        | Description                                                                                                            |
|--------------------------------|------------------------------|------------------------------------------------------------------------------------------------------------------------|
| **Success (Serial Write)**     | **Defined & Consistent**     | All replicas are identical. The client sees exactly what they wrote.                                                   |
| **Concurrent Write (Overlap)** | **Consistent but Undefined** | All replicas are identical (consistent), but the data is a jumbled mix of fragments from multiple clients (undefined). |
| **Record Append (Success)**    | **Defined & Consistent**     | All replicas are identical. The record is atomic (no fragmentation).                                                   |
| **Record Append (Failure)**    | **Inconsistent**             | Replicas might differ (some have the record, some don't). GFS hides this by forcing a retry, leading to duplicates.    |

---

**Ready for Section 6: Fault Tolerance & High Availability?** (This covers Checksums, Shadow Masters, and Recovery).

Here is **Section 6: Fault Tolerance & High Availability**.

In GFS, failure is not an exception; it is the norm. The system is designed to keep running even when components (Master, ChunkServers, Disks) are broken.

---

## Section 6: Fault Tolerance & High Availability

### 1. High Availability (Keeping the lights on)

GFS uses two simple but powerful strategies to stay online: **Replication** and **Fast Recovery**.

#### A. Chunk Replication

* **The Rule:** Every chunk is replicated to multiple servers (Default: **3**).
* **The Benefit:** If a server crashes or a disk dies, the data is still available on 2 other machines.
* **The Master's Job:** It constantly monitors replicas. If the count drops below 3, it clones a surviving replica to a new server.

#### B. Fast Recovery

* **Master Restart:** If the Master process crashes, it is designed to restart in seconds. Since the entire state is in memory (loaded from the fast Checkpoint), it comes back online almost instantly.
* **Client Retry:** Clients and ChunkServers are designed to simply "retry" their requests if the Master is momentarily down.

---

### 2. Master Failure (The "Brain" Death)

The Master is a **Single Point of Failure**. If the machine physically burns down, GFS needs a plan.

#### A. The Operation Log (The Backup Brain)

* The Master's "Operation Log" is the only thing that matters.
* **Replication:** This log is replicated to multiple **remote machines** in real-time.
* **Failover:** If the primary Master machine dies, a monitoring system detects it (missing heartbeats) and starts a **New Master** process on one of the remote machines that has the log.

#### B. Shadow Masters (Read-Only Mode)

To improve read availability, GFS can run **Shadow Masters**.

* **Role:** They are "Read-Only" replicas of the Master.
* **Sync:** They lag slightly behind the Primary by tailing the Operation Log.
* **Use Case:** If the Primary Master is down (or busy), clients can still read metadata from Shadow Masters. They might get slightly stale info, but for many apps, that is better than no info.

---

### 3. 3. Replica Failure & Stale Data Detection

When a ChunkServer crashes and misses updates, its data becomes "Stale." The Master must detect this to 
prevent clients from reading old data.

**Detection Mechanism: Version Numbers**

The Detection Mechanism (Version Numbers):

1. **The Mutation:** Whenever the Master grants a lease for a write, it increments the Chunk Version Number (e.g., v10 → v11).
2. **The Update:** It commands all live replicas to update their stored version to v11.
3. **The Failure:** Suppose Server C is offline during this write. It remains at v10.
4. **The Restart:** When Server C restarts, it reports its inventory to the Master: "I have Chunk X, Version 10."
5. **The Verdict:** The Master sees that the current system version is v11. It instantly marks Server C's copy as "Stale" and will not send its location to clients.

---

### 4. Data Integrity (Checksums)

Hard drives are notorious for "Silent Corruption" (flipping a bit without telling anyone). Since GFS stores PetaBytes of data, this happens often.

**The Solution: Checksumming**

* **The Block:** Every 64MB chunk is divided into tiny **64KB blocks**.
* **The Checksum:** Each 64KB block has a 32-bit Checksum stored in RAM/Log.

**How it protects Reads:**

1. **Verify First:** Before a ChunkServer sends data to a client, it checks the checksum.
2. **Corruption Detected:** If the checksum mismatches:
* It returns an **Error** to the client.
* It reports the corruption to the **Master**.


3. **Healing:** The client reads from a different replica. The Master clones a good replica to replace the bad one.

**Optimization (Why it's fast):**

* **Reads:** Checksums are only verified for the specific range being read.
* **Appends:** GFS doesn't verify the whole chunk. It just incrementally updates the checksum for the last block and adds new checksums for the new blocks.

---

## Section 7: Garbage Collection & Maintenance.

### 1. Lazy Deletion (The "Trash Bin" Strategy)

#### Why do we need this?

1. **Safety (Undo):** In a system this large, accidental deletions are common. Immediate physical deletion would be irreversible. GFS provides a safety buffer.
2. **Performance:** Reclaiming physical storage requires writing to disk (updating bitmaps/inodes). Doing this synchronously while a client waits is slow. GFS batches this work during idle times.

#### The Workflow (Step-by-Step):

**Step 1: The Logical Delete (Rename)**
When a client issues a command `delete("data.log")`:

* The Master acts immediately, but it does **not** delete the file.
* It **Renames** the file to a hidden name that includes the deletion timestamp (e.g., `.trash/data.log_20260206`).
* **Result:** The file disappears from the normal namespace, but it still exists.
* *Read:* You can still read it if you know the hidden path.
* *Recover:* You can "undelete" it by renaming it back to the original name.



**Step 2: The Grace Period**
The file sits in the hidden `.trash` directory for a configurable interval (default: **3 days**).

**Step 3: The Metadata Sweep**
The Master performs a periodic background scan of its namespace (RAM).

* It identifies files in `.trash` that are older than 3 days.
* It removes the **Metadata** for these files from its RAM.
* **Crucial Point:** The Master *still* hasn't touched the physical disks on the ChunkServers. It has simply "forgotten" that these files exist. This effectively severs the link between the filename and the chunk handles.

---

### 2. Orphaned Chunk Collection (Physical Deletion)

Now that the Master has "forgotten" the chunks, they have become **Orphans**—chunks that exist on disk but have no file metadata pointing to them. The system must find and kill them.

#### The Workflow (The Heartbeat Handshake):

**Step 1: The Chunk Report**
During the regular **Heartbeat** exchange, every ChunkServer sends the Master a list of all the chunks it currently holds on its disk.

**Step 2: The Comparison**
The Master checks this list against its own **In-Memory Metadata Map**.

* *Chunk A:* "I know this one. It belongs to `user_profile.db`. Keep it."
* *Chunk B:* "I have no record of this ID." (This is likely a chunk that was part of the file deleted in the previous step).

**Step 3: The Death Sentence**
The Master replies to the ChunkServer: *"I don't know the following IDs: [Chunk B]. You are free to delete them."*

**Step 4: The Execution**
The ChunkServer receives the command and deletes the actual `.chk` files from its local Linux file system. This is when the disk space is finally freed.

---


