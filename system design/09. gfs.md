<!-- TOC -->
* [Design Google File System (GFS)](#design-google-file-system-gfs)
  * [Resources](#resources)
  * [1. The 5 Core Components](#1-the-5-core-components)
    * [A. The Chunk (The Unit of Storage)](#a-the-chunk-the-unit-of-storage)
    * [B. The Chunkserver (The Worker)](#b-the-chunkserver-the-worker)
    * [C. The Master (The Brain)](#c-the-master-the-brain)
    * [D. The Client (The Library)](#d-the-client-the-library)
    * [E. The Lease (The Referee)](#e-the-lease-the-referee)
  * [2. The Life of a Write (Step-by-Step)](#2-the-life-of-a-write-step-by-step)
    * [Phase 1: Discovery (Control Plane)](#phase-1-discovery-control-plane)
    * [Phase 2: Data Transfer (Data Plane)](#phase-2-data-transfer-data-plane)
    * [Phase 3: The Commit (Serialization)](#phase-3-the-commit-serialization)
  * [3. CRITICAL INFO: Consistency & Trade-offs](#3-critical-info-consistency--trade-offs)
    * [A. Padding (The "Undefined Region")](#a-padding-the-undefined-region)
    * [B. Duplicates (The "Retry" Consequence)](#b-duplicates-the-retry-consequence)
    * [C. Defined vs. Undefined](#c-defined-vs-undefined)
  * [4. Metadata Management (RAM vs. Disk)](#4-metadata-management-ram-vs-disk)
  * [5. Summary Checklist](#5-summary-checklist-)
<!-- TOC -->

# Design Google File System (GFS)

## Resources

* [Google File System - Paper that inspired Hadoop](https://www.youtube.com/watch?v=eRgFNW4QFDc)

---


## 1. The 5 Core Components

### A. The Chunk (The Unit of Storage)

Instead of small 4KB blocks, GFS uses massive fixed-size blocks.

* **Size:** **64 MB**.
* **Why 64MB?**
1. **Reduces Metadata:** A 1 PB file system generates a metadata map small enough to 
  fit entirely in the Master's **RAM**.
2. **High Throughput:** Clients can establish a persistent TCP connection to a chunkserver 
  and stream data for a long time, minimizing network overhead.


* **Identification:** Each chunk has a unique **64-bit Chunk Handle** (ID).

### B. The Chunkserver (The Worker)

Standard Linux machines that store the data.

* **Storage:** Chunks are stored as standard **Linux files** on local disks. (e.g., `/var/data/chunk_501`).
* **Reliability:** The chunkserver splits every 64MB chunk into 64KB blocks and 
  maintains a **32-bit Checksum** for each. On every read, it verifies the checksum to 
  detect "bit rot" (silent corruption).

### C. The Master (The Brain)

The single coordinator. It manages metadata but **never touches data bytes**.

* **Job:** Handles namespace operations (create, delete) and chunk placement decisions.
* **Single Point of Failure:** To mitigate this, GFS uses **Shadow Masters** (read-only replicas) and 
  an Operation Log.

### D. The Client (The Library)

A library linked into the application code (e.g., the Web Crawler).

* **Function:** Talks to Master for metadata, then talks directly to Chunkservers for data.
* **Intelligence:** It caches chunk locations and implements the **Data Pipeline** logic.

### E. The Lease (The Referee)

The mechanism to ensure consistency during writes.

* **The Primary:** The Master grants a 60-second "Lease" to **one** replica.
* **Role:** The Primary determines the **Serial Order** of mutations (writes) to ensure all 
  replicas apply changes identically.

---

## 2. The Life of a Write (Step-by-Step)

This flow decouples the heavy **Data Flow** from the lightweight **Control Flow**.

### Phase 1: Discovery (Control Plane)

1. **Request:** Client asks Master: *"I want to append to `/logs/foo.txt`."*
2. **Lease Assignment:** Master checks if a Primary exists. If not, it grants a **Lease** to one replica (S1).
3. **Reply:** Master returns locations: `Primary: S1`, `Secondaries: S2, S3`.

### Phase 2: Data Transfer (Data Plane)

* **Goal:** Maximize network bandwidth.
* **Pipeline:** Client sends data to the **closest** server (e.g., S2), which forwards to the next closest.
* **State:** Data sits in an **LRU Buffer (RAM)** on all 3 servers. **No disk write happens yet.**

### Phase 3: The Commit (Serialization)

1. **Trigger:** Client sends a tiny **Write Command** to the **Primary (S1)**.
2. **Serialization:** Primary assigns a **Serial Number** to the mutation.
3. **Local Write:** Primary writes data to its local disk at specific offset `X`.
4. **Remote Command:** Primary tells S2 and S3: *"Write buffered data (ID 555) at **Offset X**."*
5. **Execution:** S2 and S3 write at exactly Offset X and reply "Success."

---

## 3. CRITICAL INFO: Consistency & Trade-offs

*(This is the section that distinguishes Senior Engineers from Juniors. It explains the "weird" behaviors.)*

GFS relaxes consistency to achieve high performance. It guarantees **At-Least-Once** semantics, which 
creates two side effects:

### A. Padding (The "Undefined Region")

**Problem:** A client wants to append 2MB, but the current chunk only has 1MB of space 
left (Total 64MB). GFS refuses to split "Atomic Records" across chunks.

* **Action:** The Primary fills the remaining 1MB with **Padding Bytes** (Junk) to close the chunk.
* **Result:** It forces the Client to retry on the next chunk. The file now 
  contains "Undefined Regions" (Junk) that the reader application must be smart enough to skip.

### B. Duplicates (The "Retry" Consequence)

**Problem:** During a write, S1 and S3 succeed, but S2 fails (e.g., disk full).

* **State:** The file is inconsistent.
* **Action:** The Primary reports **FAILURE** to the Client.
* **Retry:** The Client **retries** the entire operation.
* S1 and S3 write the data **again** (creating a duplicate).
* S2 writes it for the first time.


* **Result:** S1 and S3 now have the data twice. This is acceptable in 
  GFS ("At-Least-Once"). The reader application is responsible for filtering out duplicates using IDs.

### C. Defined vs. Undefined

* **Defined:** A region of the file where all replicas are identical and contain the correct data.
* **Undefined:** A region containing inconsistent data or padding. GFS applications are designed to tolerate this.

---

## 4. Metadata Management (RAM vs. Disk)

The Master stores three types of metadata. You must know which lives where.

| Metadata Type       | Description              | Stored in RAM? | Stored on Disk?   | Why?                                                                                 |
|---------------------|--------------------------|----------------|-------------------|--------------------------------------------------------------------------------------|
| **File Namespace**  | `Filename`  `Chunk IDs`  | **YES**        | **YES** (OpLog)   | If lost, files effectively disappear. Must be persistent.                            |
| **Chunk Info**      | Version numbers & Leases | **YES**        | **YES** (Version) | Version # guards against stale replicas.                                             |
| **Chunk Locations** | `Chunk ID`  `Server IPs` | **YES**        | **NO**            | Chunkservers join/leave too often. Master asks them *"What do you have?"* on reboot. |

**The Operation Log:**
This is the Master's lifeline. It is a sequential journal of every namespace 
change (Create, Delete). If the Master crashes, it replays this log to restore its state.

---

## 5. Summary Checklist 

To recap the Write Process in an interview:

1. Setup: Master grants a **Lease** to a Primary to act as the dictator.
2. Chain: Data is pushed via a linear **Pipeline** to maximize bandwidth (Data Plane).
3. Replicas: Data sits in **RAM** buffers until triggered.
4. Logic: Primary assigns a serial number (**Serialization**) and enforces the exact write 
  offset on all secondaries (Control Plane).