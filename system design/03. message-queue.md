<!-- TOC -->
* [How to Design a Distributed Messaging System like Kafka?](#how-to-design-a-distributed-messaging-system-like-kafka)
  * [Resources](#resources)
  * [High-Level Components](#high-level-components)
  * [Step 0: What we are trying to store (clarify the object)](#step-0-what-we-are-trying-to-store-clarify-the-object)
  * [Step 1: Topic as a logical namespace](#step-1-topic-as-a-logical-namespace)
  * [Step 2: Why a topic cannot be a single file](#step-2-why-a-topic-cannot-be-a-single-file)
  * [Step 3: Partition = unit of storage](#step-3-partition--unit-of-storage)
  * [Step 4: How a partition is stored physically](#step-4-how-a-partition-is-stored-physically)
  * [Step 5: Partition file layout (important but simple)](#step-5-partition-file-layout-important-but-simple)
  * [Step 6: Offset assignment (critical)](#step-6-offset-assignment-critical)
  * [Step 7: Where partitions live (distributed aspect)](#step-7-where-partitions-live-distributed-aspect)
  * [Step 8: Metadata required to make this work](#step-8-metadata-required-to-make-this-work)
  * [One-sentence summary (lock this in)](#one-sentence-summary-lock-this-in)
  * [Step 9: ZooKeeper Fundamentals](#step-9-zookeeper-fundamentals)
    * [What ZooKeeper is (first principles)](#what-zookeeper-is-first-principles)
    * [ZooKeeper Data Model](#zookeeper-data-model)
    * [Persistent vs Ephemeral znodes (critical)](#persistent-vs-ephemeral-znodes-critical)
      * [Persistent znodes](#persistent-znodes)
      * [Ephemeral znodes](#ephemeral-znodes)
    * [Watches (how ZooKeeper notifies changes)](#watches-how-zookeeper-notifies-changes)
    * [ZooKeeper Guarantees (important to say explicitly)](#zookeeper-guarantees-important-to-say-explicitly)
  * [Step 10: Kafka Controller (Decision Maker)](#step-10-kafka-controller-decision-maker)
    * [What is the Controller?](#what-is-the-controller)
    * [What the Controller does](#what-the-controller-does)
    * [ZooKeeper vs Controller (mental model)](#zookeeper-vs-controller-mental-model)
    * [How metadata flows (end-to-end)](#how-metadata-flows-end-to-end)
  * [Step 11: Kafka Coordination & Metadata Flow – A Complete Explanation (Corrected)](#step-11-kafka-coordination--metadata-flow--a-complete-explanation-corrected)
    * [Mental Model (Anchor This First)](#mental-model-anchor-this-first)
    * [Case 1 - Broker Startup & Controller Election (`/controller`)](#case-1---broker-startup--controller-election-controller)
      * [Step 1.1: Broker starts and connects to ZooKeeper](#step-11-broker-starts-and-connects-to-zookeeper)
      * [Step 1.2: Broker registers itself (liveness)](#step-12-broker-registers-itself-liveness)
      * [Step 1.3: Controller election (`/controller`)](#step-13-controller-election-controller)
      * [Step 1.4: Watches (corrected and precise)](#step-14-watches-corrected-and-precise)
    * [Case 2 - How Metadata Is Built Inside Brokers](#case-2---how-metadata-is-built-inside-brokers)
    * [Case 3 - Creating a New Topic (Clarified)](#case-3---creating-a-new-topic-clarified)
      * [Step 3.1: Admin sends CreateTopic request](#step-31-admin-sends-createtopic-request)
      * [Step 3.2: Broker forwards request to controller](#step-32-broker-forwards-request-to-controller)
      * [Step 3.3: Controller computes partition placement](#step-33-controller-computes-partition-placement)
      * [Step 3.4: Controller persists metadata to ZooKeeper](#step-34-controller-persists-metadata-to-zookeeper)
      * [Step 3.5: Controller notifies brokers (important correction)](#step-35-controller-notifies-brokers-important-correction)
    * [Case 4 - Broker Goes Down](#case-4---broker-goes-down)
      * [Step 4.1: Broker crashes](#step-41-broker-crashes)
      * [Step 4.2: ZooKeeper detects failure](#step-42-zookeeper-detects-failure)
      * [Step 4.3: Controller reacts](#step-43-controller-reacts)
      * [Step 4.4: Leader re-election](#step-44-leader-re-election)
      * [Step 4.5: Metadata update and propagation](#step-45-metadata-update-and-propagation)
      * [Step 4.6: Client recovery](#step-46-client-recovery)
    * [Case 5 - How Producers Know Where to Send Data](#case-5---how-producers-know-where-to-send-data)
      * [Step 5.1: Bootstrap phase](#step-51-bootstrap-phase)
      * [Step 5.2: Metadata request](#step-52-metadata-request)
      * [Step 5.3: Broker responds from local cache](#step-53-broker-responds-from-local-cache)
      * [Step 5.4: Partition selection](#step-54-partition-selection)
      * [Step 5.5: Direct write to leader](#step-55-direct-write-to-leader)
    * [Case 6 - Why Producers Never Talk to the Controller](#case-6---why-producers-never-talk-to-the-controller)
    * [7. Final Durable Mental Model (Corrected)](#7-final-durable-mental-model-corrected)
    * [Final interview-grade one-liner](#final-interview-grade-one-liner)
  * [Step 12: How producers decide which partition to write to](#step-12-how-producers-decide-which-partition-to-write-to)
    * [First principle (very important)](#first-principle-very-important)
    * [There are only three valid strategies](#there-are-only-three-valid-strategies)
      * [1. Key-based partitioning (most important)](#1-key-based-partitioning-most-important)
      * [2. Round-robin partitioning (no key)](#2-round-robin-partitioning-no-key)
      * [3. Explicit partition (advanced / rare)](#3-explicit-partition-advanced--rare)
    * [Important correction (this matters)](#important-correction-this-matters)
    * [Why partition choice MUST be deterministic](#why-partition-choice-must-be-deterministic)
    * [What happens if partitions increase later?](#what-happens-if-partitions-increase-later)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize)
  * [Step 13: How partitions are assigned to brokers (and leaders are chosen)](#step-13-how-partitions-are-assigned-to-brokers-and-leaders-are-chosen)
    * [Step 13.1: Brokers (what they are)](#step-131-brokers-what-they-are)
    * [Step 13.2: Partition → broker mapping](#step-132-partition--broker-mapping)
    * [Step 13.3: Why a leader is required](#step-133-why-a-leader-is-required)
    * [Step 13.4: What followers are (preview)](#step-134-what-followers-are-preview)
    * [Step 13.5: How partition placement is decided](#step-135-how-partition-placement-is-decided)
    * [Step 13.6: How producers use this information](#step-136-how-producers-use-this-information)
    * [Step 13.7: What happens if a leader fails (high-level)](#step-137-what-happens-if-a-leader-fails-high-level)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize-1)
  * [Step 14: Replication — how partitions survive failures](#step-14-replication--how-partitions-survive-failures)
    * [Step 14.1: Replication factor (RF)](#step-141-replication-factor-rf)
    * [Step 14.2: Leader and followers](#step-142-leader-and-followers)
    * [Step 14.3: How replication works (mechanically)](#step-143-how-replication-works-mechanically)
      * [Followers replicate](#followers-replicate)
    * [Step 14.4: In-Sync Replicas (ISR)](#step-144-in-sync-replicas-isr)
    * [Step 14.5: Why ISR exists](#step-145-why-isr-exists)
    * [Step 14.6: Acknowledging writes (high-level)](#step-146-acknowledging-writes-high-level)
    * [Step 14.7: Leader failure and recovery](#step-147-leader-failure-and-recovery)
    * [Step 14.8: Why leader must be chosen from ISR](#step-148-why-leader-must-be-chosen-from-isr)
    * [Step 14.9: What replication guarantees](#step-149-what-replication-guarantees)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize-2)
  * [Step 15: High-Water Mark (HWM) — why consumers don't see unsafe data](#step-15-high-water-mark-hwm--why-consumers-dont-see-unsafe-data)
    * [Step 15.1: The problem illustrated](#step-151-the-problem-illustrated)
    * [Step 15.2: What can go wrong without protection](#step-152-what-can-go-wrong-without-protection)
    * [Step 15.3: Kafka’s solution — High-Water Mark](#step-153-kafkas-solution--high-water-mark)
    * [Step 15.4: Visibility rule (this is the key)](#step-154-visibility-rule-this-is-the-key)
    * [Step 15.5: How HWM moves forward](#step-155-how-hwm-moves-forward)
    * [Step 15.6: Why this guarantees safety](#step-156-why-this-guarantees-safety)
    * [Step 15.7: Important clarification (very common confusion)](#step-157-important-clarification-very-common-confusion)
    * [Step 15.8: Producer acknowledgments vs HWM (brief preview)](#step-158-producer-acknowledgments-vs-hwm-brief-preview)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize-3)
  * [Step 16: Producer acknowledgments (acks) — write safety vs performance](#step-16-producer-acknowledgments-acks--write-safety-vs-performance)
    * [First principle (anchor this)](#first-principle-anchor-this)
    * [There are only 3 valid acks settings](#there-are-only-3-valid-acks-settings)
    * [1. acks = 0 (fire-and-forget)](#1-acks--0-fire-and-forget)
    * [2. acks = 1 (leader-only acknowledgment)](#2-acks--1-leader-only-acknowledgment)
    * [3. acks = all (ISR acknowledgment)](#3-acks--all-isr-acknowledgment)
    * [How acks and HWM work together (THIS is key)](#how-acks-and-hwm-work-together-this-is-key)
    * [Why Kafka separates these concerns](#why-kafka-separates-these-concerns)
    * [Real-world defaults (important)](#real-world-defaults-important)
    * [Common interview pitfall (avoid this)](#common-interview-pitfall-avoid-this)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize-4)
  * [Step 17: Consumer Groups, Partition Assignment, and Rebalancing](#step-17-consumer-groups-partition-assignment-and-rebalancing)
    * [Step 17.1: Why consumers need coordination](#step-171-why-consumers-need-coordination)
    * [Step 17.2: Consumer group (what it really is)](#step-172-consumer-group-what-it-really-is)
    * [Step 17.3: Partition assignment rule (core rule)](#step-173-partition-assignment-rule-core-rule)
    * [Step 17.4: Why partitions ≥ consumers matters](#step-174-why-partitions--consumers-matters)
    * [Step 17.5: Offset tracking (important recap)](#step-175-offset-tracking-important-recap)
    * [Step 17.6: What is rebalancing?](#step-176-what-is-rebalancing)
    * [Step 17.7: Rebalancing — step by step](#step-177-rebalancing--step-by-step)
    * [Step 17.8: Why rebalance pauses consumption](#step-178-why-rebalance-pauses-consumption)
    * [Step 17.9: Who coordinates this?](#step-179-who-coordinates-this)
    * [Step 17.10: Failure handling](#step-1710-failure-handling)
    * [Step 17.11: Why Kafka chose this model](#step-1711-why-kafka-chose-this-model)
    * [One-sentence summary (memorize)](#one-sentence-summary-memorize-5)
  * [ZooKeeper and Controller in Kafka](#zookeeper-and-controller-in-kafka)
    * [Big Picture (start here)](#big-picture-start-here)
    * [ZooKeeper — What it REALLY is (ELI5)](#zookeeper--what-it-really-is-eli5)
    * [What Kafka stores in ZooKeeper](#what-kafka-stores-in-zookeeper)
    * [❌ What ZooKeeper never stores (very important)](#-what-zookeeper-never-stores-very-important)
    * [Controller — What it REALLY is (ELI5)](#controller--what-it-really-is-eli5)
    * [What the Controller DOES](#what-the-controller-does-1)
    * [What the Controller does NOT do](#what-the-controller-does-not-do)
    * [ZooKeeper vs Controller (mental model)](#zookeeper-vs-controller-mental-model-1)
    * [End-to-End Failure Scenario (ELI5)](#end-to-end-failure-scenario-eli5)
    * [Why this design is GOOD (interview insight)](#why-this-design-is-good-interview-insight)
    * [Common Interview Traps (avoid these)](#common-interview-traps-avoid-these)
    * [One-Sentence Summary (memorize)](#one-sentence-summary-memorize-6)
  * [Storing Metadata in ZooKeeper view](#storing-metadata-in-zookeeper-view)
    * [1 How partition metadata is REALLY stored (ZooKeeper view)](#1-how-partition-metadata-is-really-stored-zookeeper-view)
    * [2. How broker liveness is stored](#2-how-broker-liveness-is-stored)
    * [3. Where the Controller fits in (important)](#3-where-the-controller-fits-in-important)
    * [4. Do producers read ZooKeeper directly?](#4-do-producers-read-zookeeper-directly)
    * [5. Then how does a producer find the leader?](#5-then-how-does-a-producer-find-the-leader)
    * [6. What happens when leader changes?](#6-what-happens-when-leader-changes)
    * [7. Final mental model (lock this in)](#7-final-mental-model-lock-this-in)
    * [One-sentence interview answer (memorize)](#one-sentence-interview-answer-memorize)
  * [What's the format of Log Files](#whats-the-format-of-log-files)
    * [1. What is the actual format of partition log files?](#1-what-is-the-actual-format-of-partition-log-files)
    * [2. Where is the offset stored after a leader writes a message?](#2-where-is-the-offset-stored-after-a-leader-writes-a-message)
<!-- TOC -->

# How to Design a Distributed Messaging System like Kafka?

## Resources

* [System Design - Distributed Messaging Queue | Design Messaging Queue like Kafka, RabbitMQ](https://www.youtube.com/watch?v=oVZtzZVe9Dg)
* [Kafka: How to Design a Distributed Messaging System?](https://www.designgurus.io/course-play/grokking-the-advanced-system-design-interview/doc/messaging-systems-introduction)


## High-Level Components

1. Topic vs Partition
2. Write Path (Producer → Leader)
3. Storage Format
4. Replication & ISR
5. Safety: High-Water Mark & ACKs
6. Metadata, ZooKeeper & Controller

## Step 0: What we are trying to store (clarify the object)

A messaging system stores **messages**, but not randomly.

Each message has:

* topic
* order
* durability requirement

So the **primary abstraction** is:
> An ordered, durable stream of messages
>

## Step 1: Topic as a logical namespace

What is a Topic?

A topic is NOT storage.

A topic is just:

* a name
* a logical grouping of messages

Example:

```text
orders
payments
logs
```

At this stage:

* topic has no location
* topic has no file
* topic has no machine

Think of it as a **directory name**, not a file.


## Step 2: Why a topic cannot be a single file

Suppose we store topic `orders` in one file:

```text
orders.log
```

Problems:

* single machine bottleneck
* limited throughput
* single disk
* no parallel writes
* no parallel reads

So we must split the topic.


## Step 3: Partition = unit of storage

What is a Partition?

> A partition is the actual storage unit.

Each partition is:

* append-only
* ordered
* stored independently

So a topic becomes:

```text
Topic: orders
  ├── Partition 0
  ├── Partition 1
  └── Partition 2
```

Now:

* partitions can live on different machines
* writes scale
* reads scale


## Step 4: How a partition is stored physically

Now we get concrete.

Each **partition is stored as a log**.

That means:
* new messages are appended
* nothing is modified in the middle

Example partition log:

```text
P0.log
[ offset 0 ][ offset 1 ][ offset 2 ][ offset 3 ] ...
```

This is the core design decision.

Why?

* sequential disk writes
* OS page cache friendly
* crash-safe


## Step 5: Partition file layout (important but simple)

A partition is not one giant file forever.

Instead:

```text
Partition P0
  ├── segment-000000.log
  ├── segment-000128.log
  ├── segment-000256.log
```

Each segment:

* fixed size (e.g., 1GB)
* append-only
* immutable once closed

Why segments?

* easier deletion (retention)
* faster recovery
* bounded file size


## Step 6: Offset assignment (critical)

Offsets are:

* per partition
* monotonically increasing

When a message is appended:

```text
next_offset = last_offset + 1
```

Offsets are:

* logical positions
* not array indices
* never reused

This is what enables:

* replay
* consumer independence
* crash recovery


## Step 7: Where partitions live (distributed aspect)

Each partition:

* is assigned to one broker initially
* later replicated (we’ll get to that)

Example:

```text
Broker 1 → orders-P0
Broker 2 → orders-P1
Broker 3 → orders-P2
```

This is how:

* load is distributed
* throughput scales


## Step 8: Metadata required to make this work

To store topics and partitions, the system must track:

* topic name
* number of partitions
* partition → broker mapping
* partition → segment files
* current end offset

This metadata must be:

* durable
* shared across brokers

Now we pause Kafka and explain how this metadata is safely managed.

This is where ZooKeeper enters.


## One-sentence summary (lock this in)

> Topics are logical names; partitions are the actual append-only logs stored on disk, split into segments and 
> distributed across brokers for scale.
>

## Step 9: ZooKeeper Fundamentals

### What ZooKeeper is (first principles)

ZooKeeper is **not** a database and **not** part of the data path.

ZooKeeper is:

> A strongly consistent, replicated coordination service used to store **cluster metadata** and detect failures.

Think of ZooKeeper as:

> A distributed filesystem + notification mechanism

Kafka uses ZooKeeper to answer:

* Who is alive?
* Who is leader?
* What is the cluster state?

---

### ZooKeeper Data Model

ZooKeeper stores data as a **tree of znodes**:

```text
/
├── brokers
│   └── ids
│       ├── 1
│       ├── 2
│       └── 3
├── topics
│   └── orders
│       └── partitions
│           └── 0
│               └── state
└── controller
```

Each znode:

* has a small value (JSON / bytes)
* has version metadata
* can be watched for changes

---

### Persistent vs Ephemeral znodes (critical)

#### Persistent znodes

* Live until explicitly deleted
* Used for **metadata**

Example:

```text
/topics/orders
```

---

#### Ephemeral znodes

* Exist **only while the client session is alive**
* Automatically deleted on crash

Example:

```text
/brokers/ids/1
```

This is how **broker failure detection works**.

ZooKeeper does not ping brokers.

It simply observes session disappearance.

---

### Watches (how ZooKeeper notifies changes)

ZooKeeper supports **watches**:

> “Notify me when this znode or its children change.”

Kafka controller sets watches on:

```text
/brokers/ids
```

If a broker dies:

* its ephemeral node disappears
* ZooKeeper emits an event
* ZooKeeper does nothing else

ZooKeeper **never makes decisions**.

---

### ZooKeeper Guarantees (important to say explicitly)

* ✔ Strong consistency
* ✔ Ordered updates
* ✔ Linearizable writes
* ✔ Automatic failure detection


* ❌ Not high throughput
* ❌ Not a message store
* ❌ Not a decision engine

Kafka deliberately keeps ZooKeeper usage minimal.

---

## Step 10: Kafka Controller (Decision Maker)

### What is the Controller?

The controller is:

> Exactly **one Kafka broker** elected to manage cluster-wide decisions.

Other brokers are:

* normal data brokers
* standby controllers

---

### What the Controller does

The controller:

* assigns partitions to brokers
* elects partition leaders
* manages ISR changes
* reacts to broker failures
* updates metadata in ZooKeeper

The controller:
* ❌ does NOT handle reads
* ❌ does NOT handle writes
* ❌ does NOT serve clients

---

### ZooKeeper vs Controller (mental model)

| ZooKeeper       | Controller         |
|-----------------|--------------------|
| Stores state    | Decides actions    |
| Detects failure | Handles failure    |
| Passive         | Active             |
| Reliable        | Singular authority |

This separation avoids split-brain and scales well.

---

### How metadata flows (end-to-end)

```text
ZooKeeper → Controller → Brokers → Producers / Consumers
```

Clients:

* NEVER talk to ZooKeeper
* ONLY talk to brokers


## Step 11: Kafka Coordination & Metadata Flow – A Complete Explanation (Corrected)

We will answer these core questions:

1. How brokers start, how the controller is selected, and how brokers know where the controller is
2. How metadata is built and propagated
3. What happens when a new topic is created
4. What happens when a broker goes down
5. How producers know where to send data
6. Why producers never talk to the controller
7. Whether brokers store all topic metadata

---

### Mental Model (Anchor This First)

Kafka is split into **planes**:

```text
ZooKeeper  → stores facts (metadata, liveness)
Controller → makes decisions
Brokers    → store data + execute decisions
Producers  → route data using metadata
Consumers  → read data using metadata
```

**Key invariant (lock this in):**

> ZooKeeper is watched only by the controller.
> Ordinary brokers never watch topic metadata directly.


```mermaid
flowchart LR
    subgraph ZK[ZooKeeper]
        Z1[//brokers/ids/]
        Z2[//brokers/topics/]
        Z3[//controller/]
    end

    subgraph C["Controller (Kafka Broker)"]
        CTL[Controller Logic]
    end

    subgraph B[Kafka Brokers]
        B1[Broker 1]
        B2[Broker 2]
        B3[Broker 3]
    end

    subgraph P[Producers]
        PR[Producer]
    end

    subgraph CN[Consumers]
        CO[Consumer]
    end

    %% ZooKeeper watches (CORRECT)
    CTL -- watches --> Z1
    CTL -- watches --> Z2

    B1 -- watches --> Z3
    B2 -- watches --> Z3
    B3 -- watches --> Z3

    %% Controller election
    CTL -- creates --> Z3

    %% Metadata propagation
    CTL -- UpdateMetadataRequest --> B1
    CTL -- UpdateMetadataRequest --> B2
    CTL -- UpdateMetadataRequest --> B3

    %% Client metadata flow
    PR -- MetadataRequest --> B1
    B1 -- MetadataResponse --> PR

    %% Data plane
    PR -- ProduceRequest --> B2
    CO -- FetchRequest --> B2
```

---

### Case 1 - Broker Startup & Controller Election (`/controller`)

#### Step 1.1: Broker starts and connects to ZooKeeper

When a Kafka broker starts:

* It opens a ZooKeeper session
* ZooKeeper assigns a session ID
* No Kafka metadata is loaded yet

ZooKeeper tree (conceptually):

```text
/
```

---

#### Step 1.2: Broker registers itself (liveness)

Each broker creates an **ephemeral znode**:

```text
/brokers/ids/<broker_id>
```

Example:

```text
/brokers/ids/1
```

Value:

```json
{
  "host": "10.0.1.12",
  "port": 9092
}
```

Meaning:

* “Broker 1 is alive”
* If Broker 1 crashes → ZooKeeper deletes this node automatically

This is **how broker failure is detected**.

ZooKeeper does not ping brokers.
It only observes session disappearance.

---

#### Step 1.3: Controller election (`/controller`)

Every broker attempts to create:

```text
/controller
```

Properties:

* Ephemeral
* Exactly one broker can create it

The broker that succeeds becomes the **controller**.

Example value:

```json
{
  "broker_id": 1,
  "timestamp": 1700000000
}
```

**Exactly one controller exists at any time.**

All other brokers are ordinary brokers.

---

#### Step 1.4: Watches (corrected and precise)

* **Controller watches**:

  * `/brokers/ids` → broker join/leave
  * `/brokers/topics` → topic/partition metadata changes

* **Ordinary brokers watch**:

  * `/controller` → to detect controller failure

Important clarification:

> Ordinary brokers do **NOT** watch `/brokers/topics`.
> This avoids the thundering-herd problem.

ZooKeeper only **notifies**.
It never performs decisions or fan-out.

---

### Case 2 - How Metadata Is Built Inside Brokers

1. Broker starts and registers itself:
    ```text
    /brokers/ids/<broker_id>   (ephemeral)
    ```
2. Broker **waits** with an empty metadata cache.
3. **Controller**, which is watching /brokers/ids, detects the new broker.
4. Controller sends a **full cluster snapshot** to the broker via:

    ```text
    UpdateMetadataRequest
    ```

This RPC contains:

* all brokers
* all topics
* all partitions
* leaders
* replicas
* ISR
* leader epochs
* Broker populates its in-memory metadata cache from this RPC.
* Only now does the broker:
    * know which partitions it owns
    * open corresponding log directories
    * start serving requests

➡️ The controller is the sole metadata source.

---

### Case 3 - Creating a New Topic (Clarified)

#### Step 3.1: Admin sends CreateTopic request

Request goes to **any broker**.

---

#### Step 3.2: Broker forwards request to controller

Only the controller is allowed to modify cluster metadata.

---

#### Step 3.3: Controller computes partition placement

In memory, controller decides:

```text
orders-P0 → [B1 (leader), B2]
orders-P1 → [B2 (leader), B3]
orders-P2 → [B3 (leader), B1]
```

This decision is centralized to avoid inconsistency.

---

#### Step 3.4: Controller persists metadata to ZooKeeper

Controller writes metadata:

```text
/brokers/topics/orders/partitions/0/state
```

Value:

```json
{
  "leader": 1,
  "replicas": [1,2],
  "isr": [1,2],
  "leader_epoch": 1
}
```

ZooKeeper now stores the **authoritative state**.

---

#### Step 3.5: Controller notifies brokers (important correction)

ZooKeeper does **NOT** notify all brokers.

Instead:

1. ZooKeeper notifies **controller only**
2. Controller sends `UpdateMetadataRequest` RPCs to all brokers
3. Brokers:
    * create log directories
    * initialize empty logs
    * update metadata cache

Topic is now live.

---

### Case 4 - Broker Goes Down

#### Step 4.1: Broker crashes

Example: Broker 1 crashes.

---

#### Step 4.2: ZooKeeper detects failure

Because `/brokers/ids/1` is ephemeral:

* ZooKeeper deletes it
* ZooKeeper notifies **controller**

ZooKeeper takes no further action.

---

#### Step 4.3: Controller reacts

Controller:

* already has full metadata in memory
* finds partitions where leader == Broker 1

---

#### Step 4.4: Leader re-election

For each affected partition:

1. Remove dead broker from ISR
2. Choose new leader from remaining ISR
3. Increment leader epoch

---

#### Step 4.5: Metadata update and propagation

Controller:

* writes updated state to ZooKeeper
* sends `LeaderAndISRRequest` to brokers

Brokers update their in-memory metadata.

---

#### Step 4.6: Client recovery

* Producers receive NOT_LEADER errors → refresh metadata → retry
* Consumers remain safe due to HWM

---

### Case 5 - How Producers Know Where to Send Data

#### Step 5.1: Bootstrap phase

Producer configuration:

```text
bootstrap.servers = B1,B2,B3
```

These are **ordinary brokers**, not “the controller”.

---

#### Step 5.2: Metadata request

Producer connects to any broker and sends:

```text
MetadataRequest(topic=orders)
```

---

#### Step 5.3: Broker responds from local cache

Broker replies using **its in-memory metadata cache**.

Producer learns:

* partitions
* leaders
* broker addresses

No ZooKeeper involved.

---

#### Step 5.4: Partition selection

Producer computes:

```text
partition = hash(key) % partitions
```

---

#### Step 5.5: Direct write to leader

Producer sends ProduceRequest **directly to the partition leader**.

No controller.
No ZooKeeper.
No indirection.

---

### Case 6 - Why Producers Never Talk to the Controller

This is a deliberate design choice.

If producers talked to the controller:

* controller becomes a bottleneck
* leader elections slow down
* cluster stability degrades

Therefore:

> **Controller is control-plane only and invisible to clients.**

Even if a producer connects to the controller broker:

* it is treated as a normal broker
* no special APIs exist

---

### 7. Final Durable Mental Model (Corrected)

```text
Broker startup:
  → register in ZooKeeper
  → controller elected via /controller

Metadata:
  → stored in ZooKeeper
  → controller watches ZooKeeper
  → controller pushes metadata to brokers

Controller:
  → single decision-maker
  → reacts to ZooKeeper events
  → propagates metadata via RPC

Brokers:
  → store full metadata in memory
  → serve metadata to clients
  → never watch topic znodes

Producers:
  → bootstrap to any broker
  → fetch metadata from brokers
  → write directly to leaders

ZooKeeper:
  → stores facts
  → detects liveness
  → never fan-outs to brokers
```

---

### Final interview-grade one-liner

> In ZooKeeper-based Kafka, only the controller watches ZooKeeper for metadata changes; 
> it makes all coordination decisions and pushes updated metadata to brokers via RPC, while 
> brokers serve metadata to clients and producers write directly to partition leaders.



## Step 12: How producers decide which partition to write to

At this point we have:

* Topic = logical name
* Partitions = actual logs
* Each partition lives on some broker

Now the key question:
> When a producer sends a message to a topic, how does the system choose the partition?

This decision is crucial, because it determines:

* ordering
* load balancing
* scalability

### First principle (very important)
> Partition choice happens BEFORE the message is written.
>

Once written:

* the message belongs permanently to that partition
* its order is fixed relative to other messages in that partition

### There are only three valid strategies

Kafka (and any sane messaging system) allows **exactly these patterns**.

#### 1. Key-based partitioning (most important)

**Idea**

If a message has a **key**, we use it.

Rule:

```text
partition = hash(key) % number_of_partitions
```

**Example:**

```text
Topic: orders
Partitions: 3  (P0, P1, P2)

Message:
  key = order_id = 123
```

```text
hash(123) % 3 = 1
```

→ Message goes to **Partition 1**

**Why this exists**

This guarantees:
> All messages with the same key go to the same partition.
>

Which means:

* ordering per key is preserved
* stateful processing becomes possible

Example:

* all updates for order `123`
* all events for user `456`


**Trade-off**

* If one key is “hot”, one partition becomes hot
* Load may be uneven

But correctness > perfect balance.


#### 2. Round-robin partitioning (no key)

Idea

If the producer does not care about ordering per key:

* distribute messages evenly
* maximize throughput

**Example**

Messages arrive:

```text
M1 → P0
M2 → P1
M3 → P2
M4 → P0
```

**Why this exists**

* best load balancing
* avoids hot partitions
* great for logs, metrics, events

**Trade-off**

* No ordering guarantees across messages
* Related messages may go to different partitions

#### 3. Explicit partition (advanced / rare)

Producer explicitly says:

```text
send(topic="orders", partition=2)
```



Used when:

* producer knows exactly what it’s doing
* very controlled pipelines

Dangerous if misused.

### Important correction (this matters)

> The broker does NOT randomly choose partitions.

Partitioning is decided by:

* producer-side logic (partitioner)
* using cluster metadata

This avoids:

* broker bottlenecks
* extra hops

### Why partition choice MUST be deterministic

Imagine two producers sending related messages.

If partitioning were random:

* ordering breaks
* state consistency breaks

So the rule is:

> Given the same key and metadata, all producers must compute the same partition.

### What happens if partitions increase later?

This is a subtle but important point.

If you go from:

```text
3 partitions → 6 partitions
```

Then:

```text
hash(key) % partitions
```

changes.

Result:

* same key may map to a different partition
* ordering per key breaks across time

This is why:

* partition count is usually fixed early
* increasing partitions is done carefully

### One-sentence summary (memorize)

> Producers choose partitions deterministically—usually by hashing the message key—to preserve
> ordering while enabling parallelism.
> 


## Step 13: How partitions are assigned to brokers (and leaders are chosen)

At this point we have:

* Topics
* Partitions
* Producers choosing partitions

Now the question is:

> Where does a partition actually live, and who accepts writes for it?

### Step 13.1: Brokers (what they are)

A broker is just a server in the messaging cluster.

Each broker:

* has disk
* stores partition logs
* serves read/write requests

Example cluster:

```text
Broker 1
Broker 2
Broker 3
```

### Step 13.2: Partition → broker mapping

Each **partition is assigned to exactly one broker as its leader**.

Example:

```text
Topic: orders

Partition 0 → Broker 1
Partition 1 → Broker 2
Partition 2 → Broker 3
```

This mapping is **cluster metadata**.

Important:

* partitions are the unit of placement
* brokers do NOT store entire topics
* each broker stores some partitions from many topics

This mapping is authored by the controller and propagated to brokers via
`UpdateMetadataRequest`.

### Step 13.3: Why a leader is required

For **each partition**, one broker is chosen as the **leader**.

Rule:

> All writes go to the partition leader.

Why?

* single writer → total ordering
* simple offset assignment
* avoids conflicts

Without a leader:

* two brokers could assign offsets concurrently
* ordering would break


### Step 13.4: What followers are (preview)

Other brokers may store replicas of the same partition:

```text
Partition 0:
  Leader   → Broker 1
  Follower → Broker 2
  Follower → Broker 3  
```

But:

* only the leader accepts writes
* followers replicate from leader

(Followers are not in the write path for producers.)

### Step 13.5: How partition placement is decided

When a topic is created, the controller decides:

* number of partitions
* replication factor
* which brokers host replicas
* which broker is leader initially

Goal:

* spread load evenly
* avoid hot brokers
* maximize fault tolerance

Example strategy:

* round-robin placement
* rack-awareness (advanced)

The resulting assignment is written to ZooKeeper and pushed to brokers via
`UpdateMetadataRequest`.

### Step 13.6: How producers use this information

Producers:

1. Connect to a bootstrap broker
2. Send a MetadataRequest
3. Learn:
    * partition count
    * partition → leader broker mapping
4. Send writes **directly to the leader broker**

This avoids:

* central routers
* controller involvement
* extra network hops


### Step 13.7: What happens if a leader fails (high-level)

If a leader broker crashes:

* ZooKeeper detects broker death
* Controller elects a new leader from ISR
* Controller updates metadata
* Controller sends UpdateMetadataRequest to brokers
* Producers receive NOT_LEADER errors
* Producers refresh metadata via MetadataRequest and retry

No manual intervention required.

(Leader election details are covered next.)


### One-sentence summary (memorize)

> Partitions are distributed across brokers, and each partition has a single leader broker that handles 
> all writes to preserve ordering.
> 

## Step 14: Replication — how partitions survive failures

At this point we have:

* Topics
* Partitions
* Brokers
* One leader per partition

Now the problem:
> What happens if the leader broker crashes?

Without replication:

* data loss
* system downtime

So replication is not optional.


### Step 14.1: Replication factor (RF)

When creating a topic, we choose:

```text
replication.factor = N
```

Example:

```text
RF = 3
```

Meaning:

* each partition exists on 3 brokers
* 1 leader
* 2 followers


### Step 14.2: Leader and followers

For a single partition:

```text
Partition P0:
Leader   → Broker 1
Follower → Broker 2
Follower → Broker 3
```

Rules:

* Leader handles **all writes**
* Followers **replicate from leader**
* Followers never accept writes from producers

This is crucial for **ordering and correctness**.


### Step 14.3: How replication works (mechanically)

**Write arrives**

1. Producer sends message to **leader**
2. Leader:
    * appends message to its local log
    * assigns next offset

Example:

```text
Leader log: [0][1][2][3]
```

#### Followers replicate

Followers:

* pull data from leader
* append the same bytes in the same order

```text
Follower logs:
[0][1][2][3]
```

Important:

* followers do NOT reassign offsets
* offsets are defined solely by the leader


### Step 14.4: In-Sync Replicas (ISR)

Not all replicas are always healthy.

Kafka defines:

> ISR = replicas that are fully caught up with the leader

Example:

```text
Leader (B1):    [0][1][2][3][4]
Follower (B2):  [0][1][2][3][4]  ← in sync
Follower (B3):  [0][1][2]        ← lagging
```

ISR = `{B1, B2}`

Lagging replicas are:

* temporarily excluded
* not trusted for safety decisions


### Step 14.5: Why ISR exists

If a follower is:

* slow
* partitioned
* overloaded

You **cannot** wait for it forever.

So the system says:

> Only ISR replicas are trusted for durability and leader election.
> 

This allows:

* progress
* availability


### Step 14.6: Acknowledging writes (high-level)

A write is considered **successful** when:

* leader has written it
* required number of ISR replicas have it

This depends on configuration (`acks`).

(Explained in the next section.)


### Step 14.7: Leader failure and recovery

Now the important part.

**Leader crashes**

```text
Leader B1 dies
```

System reaction:

* ZooKeeper detects broker failure
* Controller selects new leader from ISR
* Controller updates metadata in ZooKeeper
* Controller sends `UpdateMetadataRequest` to all brokers
* Brokers update in-memory metadata
* Producers refresh metadata and retry

Result:

* no data loss (because ISR had the data)
* minimal downtime


### Step 14.8: Why leader must be chosen from ISR

If you choose a non-ISR replica:

* it may miss messages
* consumers may see gaps
* data loss possible

This is why:
> Only ISR replicas are eligible leaders
> 


### Step 14.9: What replication guarantees

With proper configuration:

* ✔ Data survives broker failures
* ✔ Ordering is preserved
* ✔ Writes are durable
* ✔ System remains available

Trade-off:

* additional latency
* replication overhead

### One-sentence summary (memorize)

> Kafka replicates each partition across brokers with a single leader handling writes 
> and followers replicating, while ISR ensures safe leader election and durability under failures.
> 


## Step 15: High-Water Mark (HWM) — why consumers don't see unsafe data

At this point we have:

* Leader
* Followers
* ISR (in-sync replicas)
* Replication in progress

Now the **core problem**:

> A leader may accept a write that followers have not yet replicated.
If the leader crashes now, should consumers have seen that message?

If yes → **data loss on leader failover**
If no → **safer reads**

Kafka chooses safety


### Step 15.1: The problem illustrated

Assume:

```text
Partition P0
ISR = {B1 (Leader), B2 (Follower)}
```

Current logs:

```text
B1 (Leader):   [0][1][2][3][4]
B2 (Follower): [0][1][2][3]
```

Message `4`:

* exists only on the leader
* not yet replicated to all ISR members


### Step 15.2: What can go wrong without protection

If Kafka allowed consumers to read offset `4`:

1. Consumer reads message `4`
2. Leader crashes
3. Follower B2 becomes leader
4. B2 does not have message 4

Now:

* consumer has read data that no longer exists
* non-repeatable read
* correctness broken

This is **not acceptable** in a durable messaging system.


### Step 15.3: Kafka’s solution — High-Water Mark

Kafka introduces:

> High-Water Mark (HWM)
= highest offset that all ISR replicas have safely stored

In our example:

```text
Leader:   [0][1][2][3][4]
Follower: [0][1][2][3]

HWM = 3
```


### Step 15.4: Visibility rule (this is the key)

> Consumers can only read messages with offset ≤ HWM

So:

* offset 3 → visible
* offset 4 → hidden

Even though:

* leader has it
* producer may have been acknowledged (depending on acks)


### Step 15.5: How HWM moves forward

When follower catches up:

```text
Follower: [0][1][2][3][4]
```

Now all ISR replicas have offset `4`.

So:

```text
HWM = 4
```

Kafka:

* advances HWM
* exposes message 4 to consumers


### Step 15.6: Why this guarantees safety

Because:

* any message visible to consumers
* is guaranteed to exist on **all ISR replicas**

So if leader crashes:

* new leader already has all visible messages
* no data disappears


### Step 15.7: Important clarification (very common confusion)

HWM controls:

* what consumers can read

It does NOT directly control:

* what producers can write
* how fast replication happens

Those are separate concerns.


### Step 15.8: Producer acknowledgments vs HWM (brief preview)

* Producer ACKs depend on `acks` setting
* Consumer visibility depends on HWM

These two are **related but not identical**.

We'll cover producer ACKs next.


### One-sentence summary (memorize)

> The High-Water Mark ensures consumers only see messages that are safely replicated on 
> all in-sync replicas, preventing data loss on leader failure.
> 


## Step 16: Producer acknowledgments (acks) — write safety vs performance

So far, we've discussed:

* partitions
* leaders and followers
* ISR
* High-Water Mark (HWM)

Now the key question:

> When does Kafka tell the producer: “Your write succeeded”?

That answer is controlled by `acks`.


### First principle (anchor this)

> `acks` controls when the PRODUCER is unblocked.
HWM controls what the CONSUMER can see.

They are related, but **not the same thing**.

### There are only 3 valid acks settings

```text
acks = 0
acks = 1
acks = all   (or -1)
```

We'll go one by one.

### 1. acks = 0 (fire-and-forget)

**What happens**

1. Producer sends message to leader
2. Producer does NOT wait for any response
3. Producer assumes success immediately

Leader behavior:

* may write the message
* may crash before writing
* producer will never know


**Guarantees**

* ❌ No durability guarantee
* ❌ No retry possible
* ❌ Messages can be lost silently


**Why this exists**

* ultra-low latency
* metrics, logs, telemetry where loss is acceptable

**Mental model**

> "I don't care if the message arrives."
> 


### 2. acks = 1 (leader-only acknowledgment)

**What happens**

1. Producer sends message to leader
2. Leader appends message to its local log
3. Leader replies ACK
4. Producer considers write successful
5. Followers replicate asynchronously


**Failure window (important)**

If:

* leader crashes after ACK
* but before followers replicate

Then:

* message is lost
* consumers will never see it

Why?

* message never reached ISR fully
* HWM never advanced


**Guarantees**

* ✔ Fast
* ✔ Most common default
* ❌ Possible data loss on leader failure

**Mental model**

> "I trust the leader."
> 


### 3. acks = all (ISR acknowledgment)

**What happens**

1. Producer sends message to leader
2. Leader writes message locally
3. Leader waits until **all ISR replicas** have replicated the message
4. Leader sends ACK to producer

Only after this:
* message is guaranteed to survive leader failure


**Guarantees**

* ✔ Strong durability
* ✔ No data loss if ISR members stay alive
* ✔ Safe with HWM

Trade-off:

* higher latency  
* dependent on slowest ISR replica

**Mental model**
> "Don't tell me success until it's safe."
> 

### How acks and HWM work together (THIS is key)

Let's align them:

| Step              | Leader   | Followers | Producer       | Consumer     |
|-------------------|----------|-----------|----------------|--------------|
| Write arrives     | Has data | Not yet   | Waiting        | Cannot see   |
| Replicated to ISR | Has data | Has data  | ACK (acks=all) | HWM advances |
| Visible           | Safe     | Safe      | Done           | Can read     |

Important insight:

> With `acks=all`, producer ACK ≈ consumer visibility

With `acks=1`, that alignment breaks.


### Why Kafka separates these concerns

Kafka deliberately separates:

* producer success
* consumer visibility

Why?

* flexibility
* performance tuning
* different workloads have different needs


### Real-world defaults (important)

Most production systems use:

```text
acks = all
min.insync.replicas = 2
replication.factor ≥ 3
```

This ensures:

* tolerate 1 broker failure
* no data loss
* reasonable performance


### Common interview pitfall (avoid this)

❌ "acks=all means consumers can read immediately"

Wrong.

✔ Consumers read based on `HWM`, not `acks`.

### One-sentence summary (memorize)

> acks defines when a producer is told a write succeeded, while the High-Water Mark defines when that write becomes 
> visible to consumers.
> 



## Step 17: Consumer Groups, Partition Assignment, and Rebalancing

Up to now:

* Producers write to partitions
* Brokers store and replicate data
* HWM controls visibility

Now we answer:
> How do consumers safely and scalably read data?
> 


### Step 17.1: Why consumers need coordination

Imagine:

* Topic orders
* 10 partitions
* 20 consumer processes

Questions:

* Who reads which partition?
* How do we avoid two consumers reading the same partition?
* What happens if one consumer dies?

This **cannot** be solved by consumers independently.

So Kafka introduces **consumer groups**.


### Step 17.2: Consumer group (what it really is)

A consumer group is:
> A logical subscriber that cooperatively consumes a topic.

Properties:
* one offset per partition per group
* partitions are exclusively owned within a group


### Step 17.3: Partition assignment rule (core rule)

> Within a consumer group, a partition is assigned to at most one consumer at a time.
>

Example:

```text
Topic: orders
Partitions: P0 P1 P2 P3

Consumer Group G1:
  C1
  C2
```

Assignment:

```text
C1 → P0, P1
C2 → P2, P3
```

This guarantees:

* no duplicate processing
* ordering per partition


### Step 17.4: Why partitions ≥ consumers matters

If:

```text
Consumers > Partitions
```

Then:

* some consumers are idle

Kafka does this intentionally to preserve ordering


### Step 17.5: Offset tracking (important recap)

Offsets are:

* stored per consumer group
* per partition
* in Kafka itself

Each consumer:

* reads from assigned partitions
* periodically commits offsets


### Step 17.6: What is rebalancing?

Now the critical part.
> Rebalancing is the process of redistributing partitions among consumers in a group.

Triggers:

* consumer joins group
* consumer leaves / crashes
* partitions added
* configuration change


### Step 17.7: Rebalancing — step by step

Let's walk through a concrete example.

**Initial state**

```text
Topic: orders
Partitions: P0 P1 P2 P3

Group G1:
C1
C2
```

Assignment:

```text
C1 → P0 P1
C2 → P2 P3
```

**A new consumer joins**

```text
Group G1:
  C1
  C2
  C3
```

Kafka triggers rebalance.


**Rebalance steps (simplified)**

* All consumers pause consumption
* Current assignments are revoked
* Coordinator computes new assignments
* New assignments are sent to consumers
* Consumers resume from last committed offsets

New assignment:

```text
C1 → P0
C2 → P1
C3 → P2, P3
```


### Step 17.8: Why rebalance pauses consumption

Because:

* partitions must not be processed by two consumers simultaneously
* offsets must remain consistent

This pause is intentional for correctness.


### Step 17.9: Who coordinates this?

Each consumer group has a group coordinator:

* a broker elected to manage the group
* tracks members and assignments
* triggers rebalances

(We'll later connect this to ZooKeeper / KRaft.)


### Step 17.10: Failure handling

If a consumer crashes:

1. Heartbeats stop
2. Coordinator detects timeout
3. Rebalance triggered
4. Partitions reassigned
5. New consumer resumes from committed offset

Result:

* no data loss
* possible reprocessing (at-least-once)


### Step 17.11: Why Kafka chose this model

Trade-offs:

* ✔ Simple correctness model
* ✔ Horizontal scalability
* ✔ Strong ordering guarantees

Cost:

* rebalance pauses
* offset management complexity

### One-sentence summary (memorize)

> Consumer groups coordinate partition ownership so that each partition is consumed by only one 
> consumer at a time, with rebalancing ensuring fault tolerance and scalability.
> 


## ZooKeeper and Controller in Kafka

### Big Picture (start here)

Kafka is a distributed system, so it needs two things:

1. A shared source of truth about the cluster
2. A decision-maker to act when things change

Kafka solves this by splitting responsibilities:

| Component                       | Role                                |
|---------------------------------|-------------------------------------|
| **ZooKeeper**                   | Stores cluster state                |
| **Controller (a Kafka broker)** | Makes decisions based on that state |


### ZooKeeper — What it REALLY is (ELI5)

> ZooKeeper is a highly reliable notebook that all brokers can read and write.
>

ZooKeeper:

* stores metadata
* detects liveness
* provides coordination primitives

ZooKeeper does NOT:

* store messages
* assign offsets
* serve reads/writes
* route traffic


### What Kafka stores in ZooKeeper

ZooKeeper stores only metadata, such as:

**1. Broker liveness**

Each broker creates an ephemeral node:

```text
/brokers/ids/1
/brokers/ids/2
```

Ephemeral = auto-deleted if broker dies.


**2. Partition metadata**

For every partition:

* leader broker
* replica list
* ISR (In-Sync Replicas)

Example:

```text
Topic: orders
Partition 0:
Leader: Broker 1
ISR: [Broker 1, Broker 2]
```

**3. Controller election**

ZooKeeper ensures:

> Only ONE controller exists at a time

This is done using an ephemeral lock node.

### ❌ What ZooKeeper never stores (very important)

ZooKeeper does NOT store:

* messages
* offsets of messages
* log files
* consumer progress (modern Kafka)

This separation is intentional.


### Controller — What it REALLY is (ELI5)

> The Controller is one Kafka broker elected to manage the cluster.

There is:

* exactly one active controller
* others are standby

### What the Controller DOES

The controller is the decision-maker.

**1. Leader election**

If a broker fails:

* controller chooses new partition leaders
* only from ISR

**2. Failure handling**

When a broker goes down:

* controller detects it (via ZooKeeper)
* determines affected partitions
* reassigns leadership

**3. ISR maintenance**

Controller:

* removes lagging replicas from ISR
* adds them back once caught up

This is critical for **data safety**.


**4. Metadata updates**

Controller:

* writes updated metadata to ZooKeeper
* notifies brokers
* producers & consumers refresh metadata


### What the Controller does NOT do

The controller:

* ❌ does not handle client traffic
* ❌ does not read/write messages
* ❌ does not manage consumer offsets
* ❌ does not sit in data path

All data still flows directly between:


```text
Producer → Partition Leader
Consumer → Partition Leader
```


### ZooKeeper vs Controller (mental model)

| ZooKeeper       | Controller            |
|-----------------|-----------------------|
| Passive         | Active                |
| Stores state    | Acts on state         |
| No decisions    | Makes decisions       |
| Highly reliable | Exactly one at a time |


### End-to-End Failure Scenario (ELI5)

**Scenario: Broker 1 crashes**

Step 1: ZooKeeper notices

* Broker 1's ephemeral node disappears
* ZooKeeper records: "Broker 1 is gone"

ZooKeeper **does not act**.


Step 2: Controller reacts

* Controller is watching ZooKeeper
* Sees Broker 1 disappeared

Controller now **takes control**.


Step 3: Controller reassigns leaders

For each affected partition:

* looks at ISR
* picks a new leader
* updates metadata

Example:

```text
Partition P0
Old leader: Broker 1 (dead)
ISR: [Broker 2, Broker 3]

New leader → Broker 2
```

Step 4: System stabilizes

* Brokers update metadata
* Producers refresh leader info
* Consumers resume reading safely

No data loss if ISR rules were respected.


### Why this design is GOOD (interview insight)

Kafka separates concerns:

* ZooKeeper → reliability, coordination
* Controller → decision logic
* Brokers → data storage

This avoids:

* bottlenecks
* split-brain scenarios
* complex distributed locking



### Common Interview Traps (avoid these)

* ❌ "ZooKeeper selects leaders"
* ✔ Controller selects leaders


* ❌ "ZooKeeper handles replication"
* ✔ Brokers replicate data


* ❌ "Controller handles reads/writes"
* ✔ Leaders handle reads/writes


### One-Sentence Summary (memorize)

> ZooKeeper stores Kafka’s cluster metadata and detects failures, while the controller broker observes these changes 
> and makes decisions such as leader election, ISR updates, and recovery actions.
> 


## Storing Metadata in ZooKeeper view

Partition metadata is stored as key–value–like data and producers fetch it from brokers, not directly from ZooKeeper.


### 1 How partition metadata is REALLY stored (ZooKeeper view)

In ZooKeeper, Kafka stores metadata as hierarchical key–value nodes (znodes).

Think of ZooKeeper as a distributed JSON tree.


**Example: Topic orders, Partition 0**

Example: Topic orders, Partition 0

```text
/brokers/topics/orders/partitions/0/state
```

Value stored at that path (the "value")

```text
{
  "leader": 2,
  "leader_epoch": 7,
  "isr": [2, 3],
  "replicas": [1, 2, 3]
}
```

ELI5 meaning:

* `leader`: 2 → Broker 2 accepts writes
* `replicas` → where copies exist
* `isr` → replicas trusted for safety
* `leader_epoch` → versioning for correctness



### 2. How broker liveness is stored

Another concrete example.

Broker registration

```text
/brokers/ids/2
```

Value:

```text
{
  "host": "broker-2.kafka.local",
  "port": 9092
}
```

This node is ephemeral:

* if broker 2 crashes → node disappears
* ZooKeeper does not “decide”, it just reflects reality


### 3. Where the Controller fits in (important)

The controller broker:

* watches these ZooKeeper paths
* reacts to changes
* updates metadata when needed

Example:

* broker 2 dies
* /brokers/ids/2 disappears
* controller:
    * picks new leader from ISR
    * updates /state node with new leader

ZooKeeper stores state. Controller changes state


### 4. Do producers read ZooKeeper directly?

❌ NO — NEVER

This is critical.

> Producers and consumers NEVER talk to ZooKeeper.

Reasons:

* ZooKeeper is slow compared to brokers
* would not scale
* would become a bottleneck


### 5. Then how does a producer find the leader?

Step-by-step (this is interview gold)

**Step 1: Producer connects to any broker**

This is called a **bootstrap broker**.

```text
bootstrap.servers = broker1:9092,broker2:9092
```

**Step 2: Producer sends a Metadata Request**

Example (conceptual):

```text
MetadataRequest:
  topic = orders
```

**Step 3: Broker replies with metadata**

Response includes:

```text
Topic: orders
Partition 0 → Leader Broker 2
Partition 1 → Leader Broker 3
Partition 2 → Leader Broker 1
```

This metadata is served from:

* broker's **local cache**
* which was populated from ZooKeeper / KRaft


**Step 4: Producer caches metadata locally**

Producer now knows:

* partition count
* leader per partition
* replica layout


**Step 5: Producer writes directly to the leader**

```text
Producer → Broker 2 (leader of P0)
```

No ZooKeeper involved.


### 6. What happens when leader changes?

If a leader fails:

1. Controller updates metadata in ZooKeeper
2. Brokers update their local metadata cache
3. Producer gets a write error
4. Producer refreshes metadata
5. Producer retries to new leader

This is why Kafka clients are **metadata-aware**.


### 7. Final mental model (lock this in)

Storage

* ZooKeeper = metadata key–value store
* Example key:  
        ```text
        /brokers/topics/orders/partitions/0/state
        ```
* Example value:
        ```text
        leader, replicas, ISR  
        ```

Access

* Brokers read/write ZooKeeper
* Producers talk ONLY to brokers
* Metadata flows:
        ```text
        ZooKeeper → Controller → Brokers → Producers
        ```

### One-sentence interview answer (memorize)

> Partition metadata in Kafka is stored as hierarchical key–value data in ZooKeeper (or KRaft), but producers never 
> read it directly; they fetch cached metadata from brokers and write straight to the partition leader.


## What's the format of Log Files

### 1. What is the actual format of partition log files?

You already said:

> Each partition is stored as an append-only log, split into segments.

Now let's open the black box, but keep it simple.

**1.1 Segment file structure (what’s inside segment-000128.log)**

A segment file is **not just raw messages dumped one after another**.

Conceptually, it looks like this:

```text
[ Record ][ Record ][ Record ] ...
```

Each record (simplified) contains:

```text
| Offset | Timestamp | Key | Value | Headers | CRC |
```

ELI5:

* Offset → position in the partition
* Key → used for partitioning & ordering
* Value → actual message payload
* CRC → corruption detection

Offsets are **not stored as a separate table**. They are **embedded in the log records themselves**.


**1.2 Supporting index files (important but simple)**

For each `.log` file, Kafka also maintains:

```text
segment-000128.log
segment-000128.index
segment-000128.timeindex
```

What these do:

* Offset index
    Map:
    ```text
    offset → byte position in log file
    ```
    So Kafka can jump directly instead of scanning.
* Time index
    Map:
    ```text
    timestamp → offset
    ```
    Used for time-based reads and retention.

ELI5 analogy:

> Log file = book
> Index file = table of contents
>

**1.3 Why no fancy data structures?**

Kafka intentionally avoids:

* B-Trees
* Hash tables
* Random writes

Because:

* sequential disk I/O is fastest
* OS page cache does the heavy lifting
* simplicity = reliability

This aligns perfectly with your **append-only log** explanation.


### 2. Where is the offset stored after a leader writes a message?

This is a **very common interview confusion**, so clarity here is gold.

**2.1 Offset assignment (what you already said, reinforced)**

When a message arrives at the **leader**:

1. Leader looks at its local log end offset
2. Assigns:
    ```text
    next_offset = last_offset + 1
    ```
3. Appends the record to disk

That offset is now **part of the log record itself**.

There is no separate offset table.


**2.2 Is offset metadata stored in ZooKeeper?**

❌ No. Absolutely not.

ZooKeeper **never stored offsets for messages**.